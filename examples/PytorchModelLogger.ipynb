{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example with Data Logging in DataFed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from m3util.util.IO import make_folder\n",
    "from datafed_torchflow.pytorch import TorchLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters to Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer for 10 classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "\n",
    "        # Apply fully connected layers with ReLU and final output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations for data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,)\n",
    "        ),  # Normalize with mean and std of MNIST dataset\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model, loss function, and optimizer, and DataFed TorchLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"101024\"\n",
    "notebook_path = (\n",
    "    \"./PytorchModelLogger.ipynb\"\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(SimpleCNN().parameters(), lr=learning_rate)  # Adam optimizer\n",
    "\n",
    "model_dict = {\"model\": SimpleCNN(), \"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to pypi: <Fault -32500: 'RuntimeError: PyPI no longer supports the XMLRPC package_releases method. Use JSON or Simple API instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.'>\n"
     ]
    }
   ],
   "source": [
    "torchlogger = TorchLogger(\n",
    "    model_dict=model_dict,\n",
    "    DataFed_path=f\"2024_test_pytorch/delete_me/{suffix}\",\n",
    "    script_path=notebook_path,\n",
    "    input_data_shape=train_dataset[0][0].shape,\n",
    "    local_model_path=f\"examples/model/{suffix}\",\n",
    "    logging=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datafed_torchflow.utils import getNotebookMetadata\n",
    "import json\n",
    "\n",
    "def save_notebook(self):\n",
    "        \"\"\"\n",
    "        Saves the Jupyter notebook that runs the code training the model\n",
    "        \"\"\"\n",
    "        # don't upload the notebook to DataFed if it is already there. \n",
    "        # NOTE: The below method to check is a temporary solution and will be replaced with a comparison of the checksums\n",
    "        # first, check if the notebook filename is actually its DataFed ID, in which case it already exists in DataFed\n",
    "    \n",
    "        \n",
    "        \n",
    "        #------------------------#\n",
    "        #### checksum solution\n",
    "        \n",
    "        # first, make sure the notebook file is given (not None), otherwise there is no notebook specified to upload\n",
    "        # so just don't upload a notebook but proceed to saving the checkpoints as usual \n",
    "        if self.__file__ is not None:\n",
    "            \n",
    "            # check whether the notebook file name is a DataFed ID \n",
    "            if self.__file__.startswith(\"d/\"):\n",
    "                self.notebook_record_id = self.__file__\n",
    "\n",
    "            # if the notebook filename is not a DataFed ID, check if a notebook of the same name exists at the DataFed file path\n",
    "            else:\n",
    "                try:\n",
    "                    # this will fail if it doesn't find a match, meaning that the notebook does not already exists on DataFed\n",
    "                    self.notebook_record_id = (\n",
    "                        self.df_api.get_notebook_DataFed_ID_from_path_and_title(\n",
    "                            self.__file__\n",
    "                        )\n",
    "                    )\n",
    "            \n",
    "                except Exception as e:\n",
    "                    # the notebook is not already in DataFed, so upload it\n",
    "                    # output to user\n",
    "                    old_checksum = ''\n",
    "                    \n",
    "                    if self.logging:\n",
    "                        with open(self.log_file_path, \"a\") as f:\n",
    "                            timestamp = (\n",
    "                                datetime.now().astimezone().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                            )\n",
    "                            f.write(f\"\\n {timestamp} - Uploading notebook {self.__file__} to DataFed...\")\n",
    "                    \n",
    "                    \n",
    "\n",
    "            \n",
    "            # generate a checksum (and scipt path) for the notebook \n",
    "            notebook_metadata = getNotebookMetadata(self.__file__)\n",
    "            \n",
    "            # extract the checksum\n",
    "            new_checksum = notebook_metadata[\"script\"][\"checksum\"]\n",
    "            \n",
    "            \n",
    "            \n",
    "            # if the notebook has a DataFed record ID, extract the checksum and compare to the new checksum  \n",
    "            if self.notebook_record_id is not None:\n",
    "                notebook_DataFed_metadata = json.loads(self.df_api.dataView(self.notebook_record_id)[0].data[0].metadata)\n",
    "                old_checksum = notebook_DataFed_metadata[\"script\"][\"checksum\"]\n",
    "                \n",
    "            if new_checksum != old_checksum:\n",
    "                # do the uploading \n",
    "                if self.logging:\n",
    "                    with open(self.log_file_path, \"a\") as f:\n",
    "                        timestamp = (\n",
    "                            datetime.now().astimezone().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        )\n",
    "                        f.write(f\"\\n {timestamp} - Uploading notebook {self.__file__} to DataFed...\")\n",
    "                        \n",
    "\n",
    "                current_user, current_time = self.getUserClock()\n",
    "\n",
    "                notebook_metadata = notebook_metadata | {\n",
    "                    \"user\": current_user,\n",
    "                    \"timestamp\": current_time,\n",
    "                }\n",
    "\n",
    "                self.notebook_record_resp = self.df_api.data_record_create(\n",
    "                    metadata=notebook_metadata,\n",
    "                    record_title=self.__file__.split(\"/\")[-1],  # .split(\".\")[0],\n",
    "                    deps=self.df_api.addDerivedFrom(self.dataset_id),\n",
    "                )\n",
    "\n",
    "                self.df_api.upload_file(\n",
    "                    self.notebook_record_resp[0].data[0].id, self.__file__\n",
    "                )\n",
    "\n",
    "                self.notebook_record_id = self.notebook_record_resp[0].data[0].id      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_notebook(torchlogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to pypi: <Fault -32500: 'RuntimeError: PyPI no longer supports the XMLRPC package_releases method. Use JSON or Simple API instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.'>\n"
     ]
    }
   ],
   "source": [
    "from datafed.CommandLib import API\n",
    "import json\n",
    "df_api = API()\n",
    "\n",
    "\n",
    "#local_model_path=f\"examples/model/{suffix}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'249fee03be3ad081c37bb3414368d38f33956afa920ca737a320e94adb134859'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(df_api.dataView('d/525620049')[0].data[0].metadata)[\"script\"][\"checksum\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: MissingLoginError: Missing login for Globus Transfer.\\nPlease run:\\n\\n  globus login\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def check_globus_endpoint(endpoint_id):\n",
    "    try:\n",
    "        # Call the Globus CLI to get the endpoint status\n",
    "        result = subprocess.run(\n",
    "            ['globus', 'endpoint', 'show', endpoint_id, '--format', 'json'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        # If the call was successful, parse the JSON output\n",
    "        if result.returncode == 0:\n",
    "            endpoint_info = json.loads(result.stdout)\n",
    "            if endpoint_info.get('is_paused', True) or not endpoint_info.get('is_connected', False):\n",
    "                return f\"Endpoint {endpoint_id} is not active\"\n",
    "            else:\n",
    "                return f\"Endpoint {endpoint_id} is active\"\n",
    "        else:\n",
    "            return f\"Error: {result.stderr}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception occurred: {str(e)}\"\n",
    "check_globus_endpoint(df_api.endpointDefaultGet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd579508-6fdb-11ef-aae2-7d6f43498e7d'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_api.endpointDefaultGet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchlogger.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorchlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_notebook_DataFed_ID_from_path_and_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorchlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/anaconda3/envs/TransformerVAE6/lib/python3.10/site-packages/datafed_torchflow/datafed.py:302\u001b[0m, in \u001b[0;36mDataFed.get_notebook_DataFed_ID_from_path_and_title\u001b[0;34m(self, notebook_filename)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mGets the DataFed ID for the Jupyter notebook from the file name and DataFed path\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    ValueError: If no item with the specified title is found\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    293\u001b[0m ls_resp_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollectionItemsList(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_id, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000000\u001b[39m)\n\u001b[1;32m    294\u001b[0m notebook_ID \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    295\u001b[0m     ls_resp_2[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;241m.\u001b[39mitem[\n\u001b[1;32m    297\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnotebook_filename\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mls_resp_2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m--> 302\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     ]\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m notebook_ID\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "\n",
    "torchlogger.df_api.get_notebook_DataFed_ID_from_path_and_title(torchlogger.__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "This function calls TorchLogger.save, which does the following:\n",
    "\n",
    "1. Saves the model checkpoint\n",
    "1. Identifies the approprate metadata for the model (including DataFed provenance dependencies)\n",
    "1. Identifies and navigates to the approprate DataFed project and collection\n",
    "1. Creates a DataFed data record with this metadata\n",
    "1. Saves the model weights file or, gets the local zip file the user specified instead in order to upload multiple files to the same DataFed data record\n",
    "1. Uploads the zip file to the DataFed data record generated in the previous steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epoch,\n",
    "    base_local_file_name,\n",
    "    local_vars,\n",
    "):\n",
    "    make_folder(base_local_file_name)  # ensure the path exists to save the weights\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    file_name = f\"MNSIT_epoch_{epoch}_loss_{loss.item():.4e}\"\n",
    "    local_file_path = f\"{base_local_file_name}/{file_name}.pkl\"\n",
    "\n",
    "    torchlogger.save(\n",
    "        file_name,\n",
    "        epoch=epoch,\n",
    "        training_loss=loss.item(),\n",
    "        local_file_path=local_file_path,\n",
    "        local_vars=local_vars,\n",
    "        model_hyperparameters={\"learning_rate\": learning_rate},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # Sum up the batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "        f\"({accuracy:.2f}%)\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the DataFed Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test the CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    local_vars = locals()\n",
    "\n",
    "    train(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epoch=epoch,\n",
    "        base_local_file_name=\"model/100124/weights\",\n",
    "        local_vars=list(local_vars.items()),\n",
    "    )\n",
    "    test(model=model, device=device, test_loader=test_loader, criterion=criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
