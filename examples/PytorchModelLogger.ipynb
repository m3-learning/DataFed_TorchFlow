{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example with Data Logging in DataFed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from m3util.util.IO import make_folder \n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src\"))\n",
    "from datafed_torchflow.pytorch import TorchLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters to Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "This function calls TorchLogger.save, which does the following:\n",
    "\n",
    "1. Saves the model checkpoint\n",
    "1. Identifies the approprate metadata for the model (including DataFed provenance dependencies)\n",
    "1. Identifies and navigates to the approprate DataFed project and collection\n",
    "1. Creates a DataFed data record with this metadata\n",
    "1. Saves the model weights file or, gets the local zip file the user specified instead in order to upload multiple files to the same DataFed data record\n",
    "1. Uploads the zip file to the DataFed data record generated in the previous steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    base_local_file_name,\n",
    "    local_vars,\n",
    "):\n",
    "    make_folder(base_local_file_name)  # ensure the path exists to save the weights\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100.* correct / len(train_loader.dataset)\n",
    "    print(f\"Train Epoch: {epoch} [ Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}%]\")\n",
    "\n",
    "    \n",
    "    file_name = f\"MNSIT_epoch_{epoch}_loss_{loss.item():.4e}\"\n",
    "    local_file_path = f\"{base_local_file_name}/{file_name}.pkl\"\n",
    "\n",
    "    torchlogger.save(\n",
    "        file_name,\n",
    "        epoch=epoch,\n",
    "        #training_loss=loss.item(),\n",
    "        local_file_path=local_file_path,\n",
    "        local_vars=local_vars,\n",
    "        model_hyperparameters={\"learning_rate\": learning_rate},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations for data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,)\n",
    "        ),  # Normalize with mean and std of MNIST dataset\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True,num_workers=1, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False ,num_workers=1, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the DataFed TorchLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to pypi: <Fault -32500: 'RuntimeError: PyPI no longer supports the XMLRPC package_releases method. Use JSON or Simple API instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.'>\n"
     ]
    }
   ],
   "source": [
    "suffix = \"111424\"\n",
    "notebook_path = (\n",
    "    \"./PytorchModelLogger.ipynb\"\n",
    ")\n",
    "\n",
    "model_dict = {\"model\": Net(), \"optimizer\": optimizer}\n",
    "\n",
    "torchlogger = TorchLogger(\n",
    "    model_dict=model_dict,\n",
    "    DataFed_path=f\"2024_test_pytorch/delete_me/{suffix}\",\n",
    "    script_path=notebook_path,\n",
    "    input_data_shape=train_dataset[0][0].shape,\n",
    "    dataset_id_or_path= [file.path for file in os.scandir(\"./data/MNIST/raw\")],\n",
    "    local_model_path=f\"examples/model/{suffix}\",\n",
    "    logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.318619\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.502127\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.375137\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.245363\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.217003\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.408580\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.205448\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.044556\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.166283\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.097629\n",
      "Train Epoch: 1 [ Train Loss: 0.3413, Train Accuracy: 89.5717%]\n",
      "\n",
      "Test set: Average loss: 0.0846, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.144933\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.127926\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.192584\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.213892\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.141942\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.066463\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.143347\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.154238\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.288350\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.134243\n",
      "Train Epoch: 2 [ Train Loss: 0.1176, Train Accuracy: 96.4933%]\n",
      "\n",
      "Test set: Average loss: 0.0532, Accuracy: 9821/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    local_vars = locals()\n",
    "   \n",
    "    train(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        base_local_file_name=\"model/111324/weights\",\n",
    "        local_vars=list(local_vars.items()),\n",
    "    )\n",
    "    test(model, device, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
