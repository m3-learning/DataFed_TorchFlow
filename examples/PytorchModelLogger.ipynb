{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example with Data Logging in DataFed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from m3util.util.IO import make_folder \n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src\"))\n",
    "from datafed_torchflow.pytorch import TorchLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters to Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "This function calls TorchLogger.save, which does the following:\n",
    "\n",
    "1. Saves the model checkpoint\n",
    "1. Identifies the approprate metadata for the model (including DataFed provenance dependencies)\n",
    "1. Identifies and navigates to the approprate DataFed project and collection\n",
    "1. Creates a DataFed data record with this metadata\n",
    "1. Saves the model weights file or, gets the local zip file the user specified instead in order to upload multiple files to the same DataFed data record\n",
    "1. Uploads the zip file to the DataFed data record generated in the previous steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    base_local_file_name,\n",
    "    local_vars,\n",
    "):\n",
    "    make_folder(base_local_file_name)  # ensure the path exists to save the weights\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100.* correct / len(train_loader.dataset)\n",
    "    print(f\"Train Epoch: {epoch} [ Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}%]\")\n",
    "\n",
    "    \n",
    "    file_name = f\"MNSIT_epoch_{epoch}_loss_{loss.item():.4e}\"\n",
    "    local_file_path = f\"{base_local_file_name}/{file_name}.pkl\"\n",
    "\n",
    "    torchlogger.save(\n",
    "        file_name,\n",
    "        epoch=epoch,\n",
    "        #training_loss=loss.item(),\n",
    "        local_file_path=local_file_path,\n",
    "        local_vars=local_vars,\n",
    "        model_hyperparameters={\"learning_rate\": learning_rate},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations for data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,)\n",
    "        ),  # Normalize with mean and std of MNIST dataset\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True,num_workers=1, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False ,num_workers=1, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the DataFed TorchLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to pypi: <Fault -32500: 'RuntimeError: PyPI no longer supports the XMLRPC package_releases method. Use JSON or Simple API instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.'>\n"
     ]
    }
   ],
   "source": [
    "suffix = \"111324\"\n",
    "notebook_path = (\n",
    "    \"./PytorchModelLogger.ipynb\"\n",
    ")\n",
    "\n",
    "model_dict = {\"model\": Net(), \"optimizer\": optimizer}\n",
    "\n",
    "torchlogger = TorchLogger(\n",
    "    model_dict=model_dict,\n",
    "    DataFed_path=f\"2024_test_pytorch/delete_me/{suffix}\",\n",
    "    script_path=notebook_path,\n",
    "    input_data_shape=train_dataset[0][0].shape,\n",
    "    dataset_id_or_path= [file.path for file in os.scandir(\"./data/MNIST/raw\")],\n",
    "    local_model_path=f\"examples/model/{suffix}\",\n",
    "    logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.318619\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.502059\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.370481\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.248667\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.211049\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.412597\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.203158\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.046017\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.151284\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.091816\n",
      "Train Epoch: 1 [ Train Loss: 0.3406, Train Accuracy: 89.6000%]\n",
      "\n",
      "Test set: Average loss: 0.0840, Accuracy: 9734/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    local_vars = locals()\n",
    "   \n",
    "    train(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        base_local_file_name=\"model/111324/weights\",\n",
    "        local_vars=list(local_vars.items()),\n",
    "    )\n",
    "    test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dictionary is : {'gfg': {'a': {'aa': 5, 'bb': {}}}, 'best': {}, 'for': {}, 'geeks': {'b': 6}}\n",
      "The dictionary after filtering is : {'gfg': {'a': {'aa': 5, 'bb': {}}}, 'geeks': {'b': 6}}\n"
     ]
    }
   ],
   "source": [
    "# Python3 code to demonstrate working of \n",
    "# Removing Nested None Dictionaries\n",
    "# Using dictionary comprehension\n",
    " \n",
    "# initializing dictionary\n",
    "test_dict = {'gfg' : {'a': {'aa':5,\"bb\":{}}}, 'best' : {}, 'for' : {}, 'geeks' : {'b' : 6}}\n",
    " \n",
    "# printing original dictionary\n",
    "print(\"The original dictionary is : \" + str(test_dict))\n",
    " \n",
    "# Removing Nested None Dictionaries\n",
    "# Using dictionary comprehension\n",
    "res = {key : val for key, val in test_dict.items() if val}\n",
    " \n",
    "# printing result \n",
    "print(\"The dictionary after filtering is : \" + str(res)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': {'c': 123, 'e': {'h': [1, 2]}}, 'k': ('text',), 'm': {'n': 0, 'o': False}}\n"
     ]
    }
   ],
   "source": [
    "def clean_empty(data):\n",
    "    if isinstance(data, dict):\n",
    "        # Recursively clean each item in the dictionary\n",
    "        return {k: clean_empty(v) for k, v in data.items() if not is_empty(v)}\n",
    "    elif isinstance(data, list):\n",
    "        # Recursively clean each item in the list and remove any empty entries\n",
    "        cleaned_list = [clean_empty(item) for item in data if not is_empty(item)]\n",
    "        return cleaned_list if not is_empty(cleaned_list) else []\n",
    "    elif isinstance(data, tuple):\n",
    "        # Recursively clean each item in the tuple and remove any empty entries\n",
    "        cleaned_tuple = tuple(clean_empty(item) for item in data if not is_empty(item))\n",
    "        return cleaned_tuple if not is_empty(cleaned_tuple) else ()\n",
    "    else:\n",
    "        # Return the item as it is if it's not a list, dict, or tuple\n",
    "        return data\n",
    "\n",
    "def is_empty(value):\n",
    "    \"\"\"Helper function to determine if a value should be considered 'empty'.\"\"\"\n",
    "    if value == \"\" or value == {} or value == [] or value == ():\n",
    "        return True\n",
    "    elif isinstance(value, list) or isinstance(value, tuple):\n",
    "        # Check if all elements in a list or tuple are empty\n",
    "        return all(is_empty(item) for item in value)\n",
    "    elif isinstance(value, dict):\n",
    "        # Check if all values in a dictionary are empty\n",
    "        return all(is_empty(v) for v in value.values())\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    \"a\": \"\",\n",
    "    \"b\": {\n",
    "        \"c\": 123,\n",
    "        \"d\": [],\n",
    "        \"e\": {\n",
    "            \"f\": \"\",\n",
    "            \"g\": {},\n",
    "            \"h\": [1, 2, {}],\n",
    "            \"i\": [{}],\n",
    "            \"j\": ([], {}, \"\")\n",
    "        }\n",
    "    },\n",
    "    \"k\": ([], \"\", {\"l\": []}, \"text\"),\n",
    "    \"m\": {\"n\": 0, \"o\": False}\n",
    "}\n",
    "\n",
    "cleaned_data = clean_empty(data)\n",
    "print(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"test\"]'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps((\"test\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model Parameters': {'Model Hyperparameters': {'KL_loss_1': 0.0009508981602266431, 'KL_loss_2': 0.0, 'Contras_loss': 0.0008689357782714069, 'Maxi_loss': 0.00010871445556404069, 'test_loss': 0.22996385395526886}, 'Model Architecture': {'vae': {'layers': {'1-enc': {'enc': {'type': 'VisionTransformer', 'layer_name': 'enc', 'config': {'training': False, 'image_size': 256, 'patch_size': 16, 'hidden_dim': 16, 'mlp_dim': 3072, 'attention_dropout': 0.0, 'dropout': 0.0, 'num_classes': 1000, 'representation_size': None, 'seq_length': 257}}, 'conv_proj': {'type': 'Conv2d', 'layer_name': 'enc.conv_proj', 'config': {'training': False, 'in_channels': 1, 'out_channels': 16, 'kernel_size': [16, 16], 'stride': [16, 16], 'padding': [0, 0], 'dilation': [1, 1], 'transposed': False, 'output_padding': [0, 0], 'groups': 1, 'padding_mode': 'zeros'}}, 'encoder': {'type': 'Encoder', 'layer_name': 'enc.encoder', 'config': {'training': False}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'layers': {'type': 'Sequential', 'layer_name': 'enc.encoder.layers', 'config': {'training': False}, 'encoder_layer_0': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_0', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_0.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_0.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_0.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_0.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_0.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_1': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_1', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_1.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_1.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_1.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_1.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_1.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_2': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_2', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_2.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_2.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_2.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_2.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_2.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_3': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_3', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_3.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_3.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_3.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_3.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_3.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_4': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_4', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_4.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_4.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_4.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_4.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_4.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_5': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_5', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_5.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_5.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_5.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_5.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_5.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_6': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_6', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_6.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_6.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_6.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_6.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_6.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_7': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_7', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_7.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_7.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_7.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_7.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_7.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_8': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_8', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_8.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_8.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_8.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_8.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_8.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_9': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_9', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_9.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_9.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_9.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_9.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_9.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_10': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_10', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_10.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_10.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_10.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_10.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_10.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_11': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_11', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_11.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_11.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_11.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_11.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_11.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_12': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_12', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_12.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_12.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_12.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_12.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_12.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_13': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_13', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_13.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_13.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_13.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_13.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_13.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_14': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_14', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_14.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_14.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_14.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_14.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_14.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_15': {'type': 'EncoderBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_15', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_15.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'enc.encoder.layers.encoder_layer_15.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'enc.encoder.layers.encoder_layer_15.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_15.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.layers.encoder_layer_15.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}}, 'ln': {'type': 'LayerNorm', 'layer_name': 'enc.encoder.ln', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}}, 'heads': {'type': 'Identity', 'layer_name': 'enc.heads', 'config': {'training': False}}}, '2-dec': {'dec': {'type': 'PatchDecoder', 'layer_name': 'dec', 'config': {'training': False, 'image_size': 256, 'patch_size': 16, 'num_patches': 256, 'emb_dim': 16}}, 'proj': {'type': 'Linear', 'layer_name': 'dec.proj', 'config': {'training': False, 'in_features': 16, 'out_features': 65536}}, 'patch_to_img': {'type': 'Sequential', 'layer_name': 'dec.patch_to_img', 'config': {'training': False}, '0': {'type': 'PixelShuffle', 'layer_name': 'dec.patch_to_img.0', 'config': {'training': False, 'upscale_factor': 1}}, '1': {'type': 'ConvTranspose2d', 'layer_name': 'dec.patch_to_img.1', 'config': {'training': False, 'in_channels': 1, 'out_channels': 1, 'kernel_size': [3, 3], 'stride': [1, 1], 'padding': [1, 1], 'dilation': [1, 1], 'transposed': True, 'output_padding': [0, 0], 'groups': 1, 'padding_mode': 'zeros'}}}}, '3-mn': {'mn': {'type': 'Linear', 'layer_name': 'mn', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, '4-var': {'var': {'type': 'Linear', 'layer_name': 'var', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, '5-relu_1': {'relu_1': {'type': 'ReLU', 'layer_name': 'relu_1', 'config': {'training': False, 'inplace': False}}}}, 'training': False, 'num_vae': 1, 'fix_channel': False, 'channel_list': None, 'model_type': '4DSTEM', 'embedding_size': 16, 'set_topK': False, 'topk': 2}, 'encoder': {'layers': {'1-conv_proj': {'conv_proj': {'type': 'Conv2d', 'layer_name': 'conv_proj', 'config': {'training': False, 'in_channels': 1, 'out_channels': 16, 'kernel_size': [16, 16], 'stride': [16, 16], 'padding': [0, 0], 'dilation': [1, 1], 'transposed': False, 'output_padding': [0, 0], 'groups': 1, 'padding_mode': 'zeros'}}}, '2-encoder': {'encoder': {'type': 'Encoder', 'layer_name': 'encoder', 'config': {'training': False}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'layers': {'type': 'Sequential', 'layer_name': 'encoder.layers', 'config': {'training': False}, 'encoder_layer_0': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_0', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_0.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_0.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_0.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_0.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_0.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_0.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_0.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_0.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_0.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_0.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_0.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_1': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_1', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_1.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_1.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_1.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_1.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_1.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_1.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_1.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_1.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_1.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_1.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_1.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_2': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_2', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_2.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_2.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_2.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_2.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_2.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_2.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_2.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_2.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_2.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_2.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_2.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_3': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_3', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_3.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_3.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_3.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_3.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_3.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_3.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_3.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_3.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_3.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_3.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_3.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_4': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_4', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_4.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_4.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_4.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_4.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_4.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_4.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_4.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_4.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_4.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_4.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_4.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_5': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_5', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_5.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_5.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_5.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_5.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_5.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_5.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_5.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_5.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_5.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_5.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_5.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_6': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_6', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_6.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_6.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_6.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_6.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_6.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_6.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_6.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_6.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_6.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_6.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_6.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_7': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_7', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_7.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_7.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_7.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_7.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_7.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_7.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_7.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_7.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_7.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_7.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_7.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_8': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_8', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_8.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_8.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_8.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_8.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_8.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_8.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_8.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_8.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_8.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_8.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_8.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_9': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_9', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_9.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_9.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_9.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_9.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_9.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_9.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_9.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_9.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_9.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_9.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_9.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_10': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_10', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_10.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_10.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_10.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_10.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_10.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_10.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_10.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_10.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_10.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_10.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_10.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_11': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_11', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_11.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_11.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_11.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_11.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_11.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_11.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_11.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_11.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_11.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_11.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_11.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_12': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_12', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_12.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_12.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_12.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_12.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_12.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_12.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_12.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_12.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_12.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_12.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_12.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_13': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_13', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_13.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_13.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_13.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_13.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_13.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_13.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_13.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_13.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_13.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_13.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_13.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_14': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_14', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_14.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_14.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_14.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_14.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_14.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_14.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_14.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_14.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_14.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_14.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_14.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}, 'encoder_layer_15': {'type': 'EncoderBlock', 'layer_name': 'encoder.layers.encoder_layer_15', 'config': {'training': False, 'num_heads': 16}, 'ln_1': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_15.ln_1', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'self_attention': {'type': 'MultiheadAttention', 'layer_name': 'encoder.layers.encoder_layer_15.self_attention', 'config': {'training': False, 'embed_dim': 16, 'kdim': 16, 'vdim': 16, 'num_heads': 16, 'dropout': 0.0, 'batch_first': True, 'head_dim': 1, 'bias_k': None, 'bias_v': None, 'add_zero_attn': False}, 'out_proj': {'type': 'NonDynamicallyQuantizableLinear', 'layer_name': 'encoder.layers.encoder_layer_15.self_attention.out_proj', 'config': {'training': False, 'in_features': 16, 'out_features': 16}}}, 'dropout': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_15.dropout', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, 'ln_2': {'type': 'LayerNorm', 'layer_name': 'encoder.layers.encoder_layer_15.ln_2', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}, 'mlp': {'type': 'MLPBlock', 'layer_name': 'encoder.layers.encoder_layer_15.mlp', 'config': {'training': False}, '0': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_15.mlp.0', 'config': {'training': False, 'in_features': 16, 'out_features': 3072}}, '1': {'type': 'GELU', 'layer_name': 'encoder.layers.encoder_layer_15.mlp.1', 'config': {'training': False, 'approximate': 'none'}}, '2': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_15.mlp.2', 'config': {'training': False, 'p': 0.0, 'inplace': False}}, '3': {'type': 'Linear', 'layer_name': 'encoder.layers.encoder_layer_15.mlp.3', 'config': {'training': False, 'in_features': 3072, 'out_features': 16}}, '4': {'type': 'Dropout', 'layer_name': 'encoder.layers.encoder_layer_15.mlp.4', 'config': {'training': False, 'p': 0.0, 'inplace': False}}}}}, 'ln': {'type': 'LayerNorm', 'layer_name': 'encoder.ln', 'config': {'training': False, 'normalized_shape': [16], 'eps': 1e-06, 'elementwise_affine': True}}}, '3-heads': {'heads': {'type': 'Identity', 'layer_name': 'heads', 'config': {'training': False}}}}, 'training': False, 'image_size': 256, 'patch_size': 16, 'hidden_dim': 16, 'mlp_dim': 3072, 'attention_dropout': 0.0, 'dropout': 0.0, 'num_classes': 1000, 'representation_size': None, 'norm_layer': {}, 'seq_length': 257}, 'decoder': {'layers': {'1-proj': {'proj': {'type': 'Linear', 'layer_name': 'proj', 'config': {'training': False, 'in_features': 16, 'out_features': 65536}}}, '2-patch_to_img': {'patch_to_img': {'type': 'Sequential', 'layer_name': 'patch_to_img', 'config': {'training': False}}, '0': {'type': 'PixelShuffle', 'layer_name': 'patch_to_img.0', 'config': {'training': False, 'upscale_factor': 1}}, '1': {'type': 'ConvTranspose2d', 'layer_name': 'patch_to_img.1', 'config': {'training': False, 'in_channels': 1, 'out_channels': 1, 'kernel_size': [3, 3], 'stride': [1, 1], 'padding': [1, 1], 'dilation': [1, 1], 'transposed': True, 'output_padding': [0, 0], 'groups': 1, 'padding_mode': 'zeros'}}}}, 'training': False, 'image_size': 256, 'patch_size': 16, 'num_patches': 256, 'emb_dim': 16}, 'optimizer': {'lr': 0.0003, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205]}}, 'folder_model': 'Transformer_VAE/4DSTEM/model_102424', 'folder_figure': 'Transformer_VAE/4DSTEM/fig_102424', 'N_EPOCHS': 62000, 'initial_epoch': 50, 'batch_size': 64, 'loadWeights': False, 'model_type': '4DSTEM', 'device': 'cuda:0', 'logging': True, 'log_folder': 'logs/Upload_to_Datafed/Transformer_VAE/4DSTEM', 'log_file': 'model_102424.txt', 'log_file_path': 'logs/Upload_to_Datafed/Transformer_VAE/4DSTEM/model_102424.txt', 'seed': 42, 'start_epoch': 0, 'num_vae': 1, 'model_architecture_names': ['vae', 'encoder', 'decoder', 'optimizer'], 'ref_mn_value': 0.1, 'ref_var_value': 0.01, 'epoch': 780, 'embedding_L1_norm': 0.000461019721115008, 'weights_file_path': 'Transformer_VAE/4DSTEM/model_102424/epoch_00780_train_loss_0.23238_KL1_0.00095_KL2_0.00000_L1_0.00046_contras_loss_0.00087_maxi_loss_0.00011_.pkl', 'image_size': [256, 256], 'embedding_file_path': 'Transformer_VAE/4DSTEM/fig_102424/epoch_00780_mse_0.22996_.png', 'zip_file_path': 'Transformer_VAE/4DSTEM/model_102424/Epoch_779_train_loss_2.3237e-01.zip', 'tb': 'Traceback (most recent call last):\\n  File \"/home/jg3837/Transformer_Beta_VAE/src/transformer_beta_vae/Transformer_VAE_train.py\", line 647, in Train\\n    torchlogger.save(f\"Epoch_{epoch}_train_loss_{format(train_loss,\\'.4e\\')}.zip\", epoch=epoch, training_loss=train_loss,\\n  File \"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src/datafed_torchflow/pytorch.py\", line 540, in save\\n    dc_resp = self.df_api.data_record_create(\\n  File \"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src/datafed_torchflow/datafed.py\", line 521, in data_record_create\\n    raise e\\n  File \"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src/datafed_torchflow/datafed.py\", line 497, in data_record_create\\n    dc_resp = self.dataCreate(\\n  File \"/home/jg3837/anaconda3/envs/TransformerVAE6/lib/python3.10/site-packages/datafed/CommandLib.py\", line 495, in dataCreate\\n    return self._mapi.sendRecv(msg)\\n  File \"/home/jg3837/anaconda3/envs/TransformerVAE6/lib/python3.10/site-packages/datafed/MessageLib.py\", line 389, in sendRecv\\n    raise Exception(\\nException: Mismatched reply. Expected 718 got 717\\n', 'current_user': 'jg3837', 'current_time': '2024-10-28 15:35:32', 'computer_info': {'cpu': {'physical_cores': 32, 'total_cores': 32, 'cpu_frequency': {'current': 3557.04875, 'min': 1500.0, 'max': 3000.0}, 'cpu_usage_per_core': [63.9, 62.2, 63.9, 64.9, 62.9, 86.5, 63.2, 62.2, 62.9, 62.2, 62.2, 61.1, 63.9, 61.1, 62.2, 61.1, 63.9, 19.4, 63.2, 64.9, 35.1, 69.4, 62.9, 70.3, 61.1, 63.9, 75.7, 63.9, 52.8, 60.0, 42.9, 77.8], 'total_cpu_usage': 61.8}, 'memory': {'total': '1007.68 GB', 'available': '539.22 GB', 'used': '462.99 GB', 'percent': 46.5}, 'gpu': [{'id': 0, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '22025.00 MB', 'memory_free': '26646.00 MB', 'load': '0.00%', 'temperature': '60.0 °C'}, {'id': 1, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '12822.00 MB', 'memory_free': '35848.00 MB', 'load': '52.00%', 'temperature': '66.0 °C'}, {'id': 2, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '2276.00 MB', 'memory_free': '46394.00 MB', 'load': '0.00%', 'temperature': '29.0 °C'}, {'id': 3, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '13.00 MB', 'memory_free': '48657.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 4, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '278.00 MB', 'memory_free': '48392.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 5, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '388.00 MB', 'memory_free': '48282.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 6, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '13.00 MB', 'memory_free': '48657.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 7, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '13.00 MB', 'memory_free': '48657.00 MB', 'load': '0.00%', 'temperature': '29.0 °C'}], 'python': {'python_version': '3.10.12', 'python_implementation': 'CPython', 'python_build': ['main', 'Jul  5 2023 18:54:27'], 'packages': {'zipp': '3.19.2', 'zict': '3.0.0', 'zenodopy': '0.3.0', 'yarl': '1.13.1', 'xrayutilities': '1.7.8', 'xlrd': '2.0.1', 'wrapt': '1.16.0', 'widgetsnbextension': '4.0.11', 'wheel': '0.43.0', 'wget': '3.2', 'werkzeug': '3.0.3', 'websocket-client': '1.8.0', 'webencodings': '0.5.1', 'webcolors': '24.6.0', 'wcwidth': '0.2.13', 'urllib3': '1.26.20', 'uri-template': '1.3.0', 'uncertainties': '3.2.2', 'uc-micro-py': '1.0.3', 'tzdata': '2024.1', 'typing-inspect': '0.9.0', 'typing-extensions': '4.12.2', 'types-requests': '2.32.0.20240712', 'types-python-dateutil': '2.9.0.20240316', 'typeguard': '4.3.0', 'triton': '3.0.0', 'traits': '6.4.3', 'traitlets': '5.14.3', 'tqdm': '4.66.4', 'tornado': '6.4.1', 'torchx': '0.7.0', 'torchvision': '0.19.0', 'torchsummary': '1.5.1', 'torchprofile': '0.0.4', 'torchmetrics': '1.4.2', 'torchaudio': '2.4.0', 'torch': '2.4.0', 'toolz': '0.12.1', 'tomli': '2.0.1', 'tinycss2': '1.3.0', 'tifffile': '2024.7.24', 'threadpoolctl': '3.5.0', 'terminado': '0.18.1', 'termcolor': '2.4.0', 'tensorly': '0.8.1', 'tensorflow': '2.17.0', 'tensorflow-io-gcs-filesystem': '0.37.1', 'tensorboard': '2.17.0', 'tensorboard-data-server': '0.7.2', 'tenacity': '9.0.0', 'tblib': '3.0.0', 'tabulate': '0.9.0', 'sympy': '1.13.1', 'stack-data': '0.6.3', 'sqlalchemy': '2.0.31', 'sphinxcontrib-serializinghtml': '2.0.0', 'sphinxcontrib-qthelp': '2.0.0', 'sphinxcontrib-jsmath': '1.0.1', 'sphinxcontrib-htmlhelp': '2.1.0', 'sphinxcontrib-devhelp': '2.0.0', 'sphinxcontrib-bibtex': '2.6.2', 'sphinxcontrib-applehelp': '2.0.0', 'sphinx': '7.4.7', 'sphinx-togglebutton': '0.3.2', 'sphinx-thebe': '0.3.1', 'sphinx-multitoc-numbering': '0.1.3', 'sphinx-jupyterbook-latex': '1.0.0', 'sphinx-external-toc': '1.0.1', 'sphinx-design': '0.6.0', 'sphinx-copybutton': '0.5.2', 'sphinx-comments': '0.0.3', 'sphinx-book-theme': '1.1.3', 'spglib': '2.5.0', 'soupsieve': '2.5', 'sortedcontainers': '2.4.0', 'snowballstemmer': '2.2.0', 'sniffio': '1.3.1', 'six': '1.16.0', 'simpy': '4.1.1', 'simpleitk': '2.3.1', 'sidpy': '0.12.3', 'setuptools': '58.2.0', 'send2trash': '1.8.3', 'seaborn': '0.13.2', 'scipy': '1.10.1', 'scikit-learn': '1.5.1', 'scikit-image': '0.20.0', 'scifireaders': '0.11.5', 'rpds-py': '0.19.1', 'rosettasciio': '0.6', 'rich': '13.7.1', 'rfc3986-validator': '0.1.1', 'rfc3339-validator': '0.1.4', 'requests': '2.32.3', 'referencing': '0.35.1', 'qtpy': '2.4.1', 'qtconsole': '5.5.2', 'pyzmq': '26.0.3', 'pyyaml': '6.0.1', 'pywget': '0.31', 'pywavelets': '1.6.0', 'pyusid': '0.0.12', 'pytz': '2024.1', 'pytorch-lightning': '2.4.0', 'python-json-logger': '2.0.7', 'python-dateutil': '2.9.0.post0', 'python-box': '7.2.0', 'pytest': '8.3.3', 'pytemlib': '0.2024.2.2', 'pysptools': '0.15.0', 'pysocks': '1.7.1', 'pyro-ppl': '1.9.1', 'pyro-api': '0.1.2', 'pyre-extensions': '0.0.31', 'pyparsing': '3.1.2', 'pynsid': '0.0.7.2', 'pyjwt': '2.9.0', 'pygments': '2.18.0', 'pydata-sphinx-theme': '0.15.4', 'pydantic': '2.9.2', 'pydantic-core': '2.23.4', 'pycroscopy': '0.63.3', 'pycparser': '2.22', 'pycodestyle': '2.12.0', 'pybtex': '0.24.0', 'pybtex-docutils': '1.0.3', 'py-cpuinfo': '9.0.0', 'pure-eval': '0.2.3', 'ptyprocess': '0.7.0', 'ptflops': '0.7.4', 'psutil': '6.0.0', 'protobuf': '4.25.5', 'prompt-toolkit': '3.0.47', 'prometheus-client': '0.20.0', 'prettytable': '3.10.2', 'pooch': '1.8.2', 'pluggy': '1.5.0', 'plotly': '5.23.0', 'platformdirs': '4.2.2', 'pip': '24.0', 'pint': '0.24.3', 'pillow': '10.4.0', 'pexpect': '4.9.0', 'partd': '1.4.2', 'parso': '0.8.4', 'pandocfilters': '1.5.1', 'pandas': '2.2.2', 'packaging': '24.1', 'overrides': '7.7.0', 'optree': '0.12.1', 'opt-einsum': '3.3.0', 'opencv-python': '4.10.0.84', 'nvidia-nvtx-cu12': '12.1.105', 'nvidia-nvjitlink-cu12': '12.5.82', 'nvidia-nccl-cu12': '2.20.5', 'nvidia-ml-py': '12.560.30', 'nvidia-cusparse-cu12': '12.1.0.106', 'nvidia-cusolver-cu12': '11.4.5.107', 'nvidia-curand-cu12': '10.3.2.106', 'nvidia-cufft-cu12': '11.0.2.54', 'nvidia-cudnn-cu12': '9.1.0.70', 'nvidia-cuda-runtime-cu12': '12.1.105', 'nvidia-cuda-nvrtc-cu12': '12.1.105', 'nvidia-cuda-cupti-cu12': '12.1.105', 'nvidia-cublas-cu12': '12.1.3.1', 'numpy': '1.26.4', 'numpy-groupies': '0.9.7', 'numba': '0.60.0', 'notebook': '7.2.1', 'notebook-shim': '0.2.4', 'ninja': '1.11.1.1', 'networkx': '3.3', 'nest-asyncio': '1.6.0', 'nbformat': '5.10.4', 'nbconvert': '7.16.4', 'nbclient': '0.10.0', 'natsort': '8.4.0', 'namex': '0.0.8', 'myst-parser': '2.0.0', 'myst-nb': '1.1.1', 'mypy-extensions': '1.0.0', 'multipledispatch': '1.0.0', 'multidict': '6.1.0', 'msgpack': '1.0.8', 'mpmath': '1.3.0', 'ml-dtypes': '0.4.0', 'mkl-service': '2.4.0', 'mkl-random': '1.2.4', 'mkl-fft': '1.3.8', 'mistune': '3.0.2', 'mdurl': '0.1.2', 'mdit-py-plugins': '0.4.1', 'matplotlib': '3.9.1', 'matplotlib-inline': '0.1.7', 'markupsafe': '2.1.5', 'markdown': '3.6', 'markdown-it-py': '3.0.0', 'm3learning-util': '2.0.3', 'm3-learning': '0.0.24', 'lxml': '5.2.2', 'locket': '1.0.0', 'lmfit': '1.3.2', 'llvmlite': '0.43.0', 'linkify-it-py': '2.0.3', 'linear-operator': '0.5.3', 'lightning-utilities': '0.11.7', 'libclang': '18.1.1', 'lazy-loader': '0.4', 'latexcodec': '3.0.0', 'kiwisolver': '1.4.5', 'keras': '3.4.1', 'jupyterlab': '4.2.4', 'jupyterlab-widgets': '3.0.11', 'jupyterlab-server': '2.27.3', 'jupyterlab-pygments': '0.3.0', 'jupyter': '1.0.0', 'jupyter-server': '2.14.2', 'jupyter-server-terminals': '0.5.3', 'jupyter-lsp': '2.2.5', 'jupyter-events': '0.10.0', 'jupyter-core': '5.7.2', 'jupyter-console': '6.6.3', 'jupyter-client': '8.6.2', 'jupyter-cache': '1.0.0', 'jupyter-book': '1.0.2', 'jsonschema': '4.23.0', 'jsonschema-specifications': '2023.12.1', 'jsonpointer': '3.0.0', 'jsonpickle': '3.3.0', 'json5': '0.9.25', 'joblib': '1.4.2', 'jmespath': '1.0.1', 'jinja2': '3.1.4', 'jedi': '0.19.1', 'jaxtyping': '0.2.19', 'isoduration': '20.11.0', 'ipywidgets': '8.1.3', 'ipython': '8.26.0', 'ipython-genutils': '0.2.0', 'ipympl': '0.9.4', 'ipykernel': '6.29.5', 'iniconfig': '2.0.0', 'importlib-metadata': '8.2.0', 'imagesize': '1.4.1', 'imageio': '2.22.3', 'idna': '3.7', 'hyperspy': '2.1.1', 'httpx': '0.27.0', 'httpcore': '1.0.5', 'hjson': '3.1.0', 'h11': '0.14.0', 'h5py': '3.11.0', 'grpcio': '1.65.2', 'greenlet': '3.0.3', 'gpytorch': '1.13', 'gputil': '1.4.0', 'google-pasta': '0.2.0', 'gmpy2': '2.1.2', 'globus-sdk': '3.44.0', 'globus-cli': '3.30.1', 'gast': '0.6.0', 'fsspec': '2024.6.1', 'frozenlist': '1.4.1', 'fqdn': '1.5.1', 'fonttools': '4.53.1', 'flexparser': '0.3.1', 'flexcache': '0.3', 'flatbuffers': '24.3.25', 'filelock': '3.15.4', 'fastjsonschema': '2.20.0', 'executing': '2.0.1', 'exceptiongroup': '1.2.2', 'docutils': '0.20.1', 'docstring-parser': '0.16', 'docker': '7.1.0', 'distributed': '2024.7.1', 'dill': '0.3.8', 'defusedxml': '0.7.1', 'deepspeed': '0.15.2', 'decorator': '5.1.1', 'debugpy': '1.8.2', 'datafed': '3.0.0', 'dask': '2024.7.1', 'cytoolz': '0.12.3', 'cycler': '0.12.1', 'cvxopt': '1.3.2', 'cryptography': '43.0.1', 'contourpy': '1.2.1', 'comm': '0.2.2', 'cloudpickle': '3.0.0', 'click': '8.1.7', 'charset-normalizer': '3.3.2', 'cffi': '1.16.0', 'certifi': '2024.7.4', 'brotli': '1.0.9', 'botorch': '0.12.0', 'bleach': '6.1.0', 'bglib': '0.0.4', 'beautifulsoup4': '4.12.3', 'babel': '2.15.0', 'ax-platform': '0.4.3', 'autophyslearn': '0.2.5', 'autopep8': '2.3.1', 'attrs': '23.2.0', 'async-timeout': '4.0.3', 'async-lru': '2.0.4', 'astunparse': '1.6.3', 'asttokens': '2.4.1', 'asteval': '1.0.1', 'ase': '3.23.0', 'arrow': '1.3.0', 'argon2-cffi': '23.1.0', 'argon2-cffi-bindings': '21.2.0', 'appdirs': '1.4.4', 'anyio': '4.4.0', 'annotated-types': '0.7.0', 'alabaster': '0.7.16', 'aiosignal': '1.3.1', 'aiohttp': '3.10.8', 'aiohappyeyeballs': '2.4.3', 'accessible-pygments': '0.0.5', 'absl-py': '2.1.0'}}}, 'metadata_file_folder': 'Transformer_VAE/4DSTEM/metadata_files', 'metadata_file': 'Epoch_779.json', 'metadata_file_path': 'Transformer_VAE/4DSTEM/metadata_files/Epoch_779.json', 'outfile': {'mode': 'w'}, 'script': {'path': '/home/jg3837/Transformer_Beta_VAE/src/transformer_beta_vae/Transformer_VAE.ipynb', 'checksum': '26f7d2a62b7968115589217341980b04d2d8f962eecc878b07b943f624c6e4a2'}, 'user': 'jg3837', 'timestamp': '2024-10-28 15:42:03'}, 'System Information': {'cpu': {'physical_cores': 32, 'total_cores': 32, 'cpu_frequency': {'current': 1713.01134375, 'min': 1500.0, 'max': 3000.0}, 'cpu_usage_per_core': [0.0, 0.0, 0.0, 0.0, 2.2, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2, 0.0, 0.0, 2.3, 0.0, 0.0, 0.0, 0.0, 60.0, 0.0, 11.4, 0.0, 2.2, 0.0, 0.0], 'total_cpu_usage': 5.8}, 'memory': {'total': '1007.68 GB', 'available': '538.85 GB', 'used': '463.35 GB', 'percent': 46.5}, 'gpu': [{'id': 0, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '22025.00 MB', 'memory_free': '26646.00 MB', 'load': '0.00%', 'temperature': '61.0 °C'}, {'id': 1, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '12822.00 MB', 'memory_free': '35848.00 MB', 'load': '0.00%', 'temperature': '68.0 °C'}, {'id': 2, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '2276.00 MB', 'memory_free': '46394.00 MB', 'load': '0.00%', 'temperature': '29.0 °C'}, {'id': 3, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '13.00 MB', 'memory_free': '48657.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 4, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '278.00 MB', 'memory_free': '48392.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 5, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '388.00 MB', 'memory_free': '48282.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 6, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '13.00 MB', 'memory_free': '48657.00 MB', 'load': '0.00%', 'temperature': '28.0 °C'}, {'id': 7, 'name': 'NVIDIA RTX A6000', 'driver_version': '550.107.02', 'memory_total': '49140.00 MB', 'memory_used': '13.00 MB', 'memory_free': '48657.00 MB', 'load': '0.00%', 'temperature': '29.0 °C'}], 'python': {'python_version': '3.10.12', 'python_implementation': 'CPython', 'python_build': ['main', 'Jul  5 2023 18:54:27'], 'packages': {'zipp': '3.19.2', 'zict': '3.0.0', 'zenodopy': '0.3.0', 'yarl': '1.13.1', 'xrayutilities': '1.7.8', 'xlrd': '2.0.1', 'wrapt': '1.16.0', 'widgetsnbextension': '4.0.11', 'wheel': '0.43.0', 'wget': '3.2', 'werkzeug': '3.0.3', 'websocket-client': '1.8.0', 'webencodings': '0.5.1', 'webcolors': '24.6.0', 'wcwidth': '0.2.13', 'urllib3': '1.26.20', 'uri-template': '1.3.0', 'uncertainties': '3.2.2', 'uc-micro-py': '1.0.3', 'tzdata': '2024.1', 'typing-inspect': '0.9.0', 'typing-extensions': '4.12.2', 'types-requests': '2.32.0.20240712', 'types-python-dateutil': '2.9.0.20240316', 'typeguard': '4.3.0', 'triton': '3.0.0', 'traits': '6.4.3', 'traitlets': '5.14.3', 'tqdm': '4.66.4', 'tornado': '6.4.1', 'torchx': '0.7.0', 'torchvision': '0.19.0', 'torchsummary': '1.5.1', 'torchprofile': '0.0.4', 'torchmetrics': '1.4.2', 'torchaudio': '2.4.0', 'torch': '2.4.0', 'toolz': '0.12.1', 'tomli': '2.0.1', 'tinycss2': '1.3.0', 'tifffile': '2024.7.24', 'threadpoolctl': '3.5.0', 'terminado': '0.18.1', 'termcolor': '2.4.0', 'tensorly': '0.8.1', 'tensorflow': '2.17.0', 'tensorflow-io-gcs-filesystem': '0.37.1', 'tensorboard': '2.17.0', 'tensorboard-data-server': '0.7.2', 'tenacity': '9.0.0', 'tblib': '3.0.0', 'tabulate': '0.9.0', 'sympy': '1.13.1', 'stack-data': '0.6.3', 'sqlalchemy': '2.0.31', 'sphinxcontrib-serializinghtml': '2.0.0', 'sphinxcontrib-qthelp': '2.0.0', 'sphinxcontrib-jsmath': '1.0.1', 'sphinxcontrib-htmlhelp': '2.1.0', 'sphinxcontrib-devhelp': '2.0.0', 'sphinxcontrib-bibtex': '2.6.2', 'sphinxcontrib-applehelp': '2.0.0', 'sphinx': '7.4.7', 'sphinx-togglebutton': '0.3.2', 'sphinx-thebe': '0.3.1', 'sphinx-multitoc-numbering': '0.1.3', 'sphinx-jupyterbook-latex': '1.0.0', 'sphinx-external-toc': '1.0.1', 'sphinx-design': '0.6.0', 'sphinx-copybutton': '0.5.2', 'sphinx-comments': '0.0.3', 'sphinx-book-theme': '1.1.3', 'spglib': '2.5.0', 'soupsieve': '2.5', 'sortedcontainers': '2.4.0', 'snowballstemmer': '2.2.0', 'sniffio': '1.3.1', 'six': '1.16.0', 'simpy': '4.1.1', 'simpleitk': '2.3.1', 'sidpy': '0.12.3', 'setuptools': '58.2.0', 'send2trash': '1.8.3', 'seaborn': '0.13.2', 'scipy': '1.10.1', 'scikit-learn': '1.5.1', 'scikit-image': '0.20.0', 'scifireaders': '0.11.5', 'rpds-py': '0.19.1', 'rosettasciio': '0.6', 'rich': '13.7.1', 'rfc3986-validator': '0.1.1', 'rfc3339-validator': '0.1.4', 'requests': '2.32.3', 'referencing': '0.35.1', 'qtpy': '2.4.1', 'qtconsole': '5.5.2', 'pyzmq': '26.0.3', 'pyyaml': '6.0.1', 'pywget': '0.31', 'pywavelets': '1.6.0', 'pyusid': '0.0.12', 'pytz': '2024.1', 'pytorch-lightning': '2.4.0', 'python-json-logger': '2.0.7', 'python-dateutil': '2.9.0.post0', 'python-box': '7.2.0', 'pytest': '8.3.3', 'pytemlib': '0.2024.2.2', 'pysptools': '0.15.0', 'pysocks': '1.7.1', 'pyro-ppl': '1.9.1', 'pyro-api': '0.1.2', 'pyre-extensions': '0.0.31', 'pyparsing': '3.1.2', 'pynsid': '0.0.7.2', 'pyjwt': '2.9.0', 'pygments': '2.18.0', 'pydata-sphinx-theme': '0.15.4', 'pydantic': '2.9.2', 'pydantic-core': '2.23.4', 'pycroscopy': '0.63.3', 'pycparser': '2.22', 'pycodestyle': '2.12.0', 'pybtex': '0.24.0', 'pybtex-docutils': '1.0.3', 'py-cpuinfo': '9.0.0', 'pure-eval': '0.2.3', 'ptyprocess': '0.7.0', 'ptflops': '0.7.4', 'psutil': '6.0.0', 'protobuf': '4.25.5', 'prompt-toolkit': '3.0.47', 'prometheus-client': '0.20.0', 'prettytable': '3.10.2', 'pooch': '1.8.2', 'pluggy': '1.5.0', 'plotly': '5.23.0', 'platformdirs': '4.2.2', 'pip': '24.0', 'pint': '0.24.3', 'pillow': '10.4.0', 'pexpect': '4.9.0', 'partd': '1.4.2', 'parso': '0.8.4', 'pandocfilters': '1.5.1', 'pandas': '2.2.2', 'packaging': '24.1', 'overrides': '7.7.0', 'optree': '0.12.1', 'opt-einsum': '3.3.0', 'opencv-python': '4.10.0.84', 'nvidia-nvtx-cu12': '12.1.105', 'nvidia-nvjitlink-cu12': '12.5.82', 'nvidia-nccl-cu12': '2.20.5', 'nvidia-ml-py': '12.560.30', 'nvidia-cusparse-cu12': '12.1.0.106', 'nvidia-cusolver-cu12': '11.4.5.107', 'nvidia-curand-cu12': '10.3.2.106', 'nvidia-cufft-cu12': '11.0.2.54', 'nvidia-cudnn-cu12': '9.1.0.70', 'nvidia-cuda-runtime-cu12': '12.1.105', 'nvidia-cuda-nvrtc-cu12': '12.1.105', 'nvidia-cuda-cupti-cu12': '12.1.105', 'nvidia-cublas-cu12': '12.1.3.1', 'numpy': '1.26.4', 'numpy-groupies': '0.9.7', 'numba': '0.60.0', 'notebook': '7.2.1', 'notebook-shim': '0.2.4', 'ninja': '1.11.1.1', 'networkx': '3.3', 'nest-asyncio': '1.6.0', 'nbformat': '5.10.4', 'nbconvert': '7.16.4', 'nbclient': '0.10.0', 'natsort': '8.4.0', 'namex': '0.0.8', 'myst-parser': '2.0.0', 'myst-nb': '1.1.1', 'mypy-extensions': '1.0.0', 'multipledispatch': '1.0.0', 'multidict': '6.1.0', 'msgpack': '1.0.8', 'mpmath': '1.3.0', 'ml-dtypes': '0.4.0', 'mkl-service': '2.4.0', 'mkl-random': '1.2.4', 'mkl-fft': '1.3.8', 'mistune': '3.0.2', 'mdurl': '0.1.2', 'mdit-py-plugins': '0.4.1', 'matplotlib': '3.9.1', 'matplotlib-inline': '0.1.7', 'markupsafe': '2.1.5', 'markdown': '3.6', 'markdown-it-py': '3.0.0', 'm3learning-util': '2.0.3', 'm3-learning': '0.0.24', 'lxml': '5.2.2', 'locket': '1.0.0', 'lmfit': '1.3.2', 'llvmlite': '0.43.0', 'linkify-it-py': '2.0.3', 'linear-operator': '0.5.3', 'lightning-utilities': '0.11.7', 'libclang': '18.1.1', 'lazy-loader': '0.4', 'latexcodec': '3.0.0', 'kiwisolver': '1.4.5', 'keras': '3.4.1', 'jupyterlab': '4.2.4', 'jupyterlab-widgets': '3.0.11', 'jupyterlab-server': '2.27.3', 'jupyterlab-pygments': '0.3.0', 'jupyter': '1.0.0', 'jupyter-server': '2.14.2', 'jupyter-server-terminals': '0.5.3', 'jupyter-lsp': '2.2.5', 'jupyter-events': '0.10.0', 'jupyter-core': '5.7.2', 'jupyter-console': '6.6.3', 'jupyter-client': '8.6.2', 'jupyter-cache': '1.0.0', 'jupyter-book': '1.0.2', 'jsonschema': '4.23.0', 'jsonschema-specifications': '2023.12.1', 'jsonpointer': '3.0.0', 'jsonpickle': '3.3.0', 'json5': '0.9.25', 'joblib': '1.4.2', 'jmespath': '1.0.1', 'jinja2': '3.1.4', 'jedi': '0.19.1', 'jaxtyping': '0.2.19', 'isoduration': '20.11.0', 'ipywidgets': '8.1.3', 'ipython': '8.26.0', 'ipython-genutils': '0.2.0', 'ipympl': '0.9.4', 'ipykernel': '6.29.5', 'iniconfig': '2.0.0', 'importlib-metadata': '8.2.0', 'imagesize': '1.4.1', 'imageio': '2.22.3', 'idna': '3.7', 'hyperspy': '2.1.1', 'httpx': '0.27.0', 'httpcore': '1.0.5', 'hjson': '3.1.0', 'h11': '0.14.0', 'h5py': '3.11.0', 'grpcio': '1.65.2', 'greenlet': '3.0.3', 'gpytorch': '1.13', 'gputil': '1.4.0', 'google-pasta': '0.2.0', 'gmpy2': '2.1.2', 'globus-sdk': '3.44.0', 'globus-cli': '3.30.1', 'gast': '0.6.0', 'fsspec': '2024.6.1', 'frozenlist': '1.4.1', 'fqdn': '1.5.1', 'fonttools': '4.53.1', 'flexparser': '0.3.1', 'flexcache': '0.3', 'flatbuffers': '24.3.25', 'filelock': '3.15.4', 'fastjsonschema': '2.20.0', 'executing': '2.0.1', 'exceptiongroup': '1.2.2', 'docutils': '0.20.1', 'docstring-parser': '0.16', 'docker': '7.1.0', 'distributed': '2024.7.1', 'dill': '0.3.8', 'defusedxml': '0.7.1', 'deepspeed': '0.15.2', 'decorator': '5.1.1', 'debugpy': '1.8.2', 'datafed': '3.0.0', 'dask': '2024.7.1', 'cytoolz': '0.12.3', 'cycler': '0.12.1', 'cvxopt': '1.3.2', 'cryptography': '43.0.1', 'contourpy': '1.2.1', 'comm': '0.2.2', 'cloudpickle': '3.0.0', 'click': '8.1.7', 'charset-normalizer': '3.3.2', 'cffi': '1.16.0', 'certifi': '2024.7.4', 'brotli': '1.0.9', 'botorch': '0.12.0', 'bleach': '6.1.0', 'bglib': '0.0.4', 'beautifulsoup4': '4.12.3', 'babel': '2.15.0', 'ax-platform': '0.4.3', 'autophyslearn': '0.2.5', 'autopep8': '2.3.1', 'attrs': '23.2.0', 'async-timeout': '4.0.3', 'async-lru': '2.0.4', 'astunparse': '1.6.3', 'asttokens': '2.4.1', 'asteval': '1.0.1', 'ase': '3.23.0', 'arrow': '1.3.0', 'argon2-cffi': '23.1.0', 'argon2-cffi-bindings': '21.2.0', 'appdirs': '1.4.4', 'anyio': '4.4.0', 'annotated-types': '0.7.0', 'alabaster': '0.7.16', 'aiosignal': '1.3.1', 'aiohttp': '3.10.8', 'aiohappyeyeballs': '2.4.3', 'accessible-pygments': '0.0.5', 'absl-py': '2.1.0'}}}}\n"
     ]
    }
   ],
   "source": [
    "with open('/home/jg3837/Transformer_Beta_VAE/src/transformer_beta_vae/Epoch_780.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adadelta.Adadelta"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model Parameters': {'Model Hyperparameters': {'KL_loss_1': 0.0009508981602266431,\n",
       "   'KL_loss_2': 0.0,\n",
       "   'Contras_loss': 0.0008689357782714069,\n",
       "   'Maxi_loss': 0.00010871445556404069,\n",
       "   'test_loss': 0.22996385395526886},\n",
       "  'Model Architecture': {'vae': {'layers': {'1-enc': {'enc': {'type': 'VisionTransformer',\n",
       "       'layer_name': 'enc',\n",
       "       'config': {'training': False,\n",
       "        'image_size': 256,\n",
       "        'patch_size': 16,\n",
       "        'hidden_dim': 16,\n",
       "        'mlp_dim': 3072,\n",
       "        'attention_dropout': 0.0,\n",
       "        'dropout': 0.0,\n",
       "        'num_classes': 1000,\n",
       "        'representation_size': None,\n",
       "        'seq_length': 257}},\n",
       "      'conv_proj': {'type': 'Conv2d',\n",
       "       'layer_name': 'enc.conv_proj',\n",
       "       'config': {'training': False,\n",
       "        'in_channels': 1,\n",
       "        'out_channels': 16,\n",
       "        'kernel_size': [16, 16],\n",
       "        'stride': [16, 16],\n",
       "        'padding': [0, 0],\n",
       "        'dilation': [1, 1],\n",
       "        'transposed': False,\n",
       "        'output_padding': [0, 0],\n",
       "        'groups': 1,\n",
       "        'padding_mode': 'zeros'}},\n",
       "      'encoder': {'type': 'Encoder',\n",
       "       'layer_name': 'enc.encoder',\n",
       "       'config': {'training': False},\n",
       "       'dropout': {'type': 'Dropout',\n",
       "        'layer_name': 'enc.encoder.dropout',\n",
       "        'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "       'layers': {'type': 'Sequential',\n",
       "        'layer_name': 'enc.encoder.layers',\n",
       "        'config': {'training': False},\n",
       "        'encoder_layer_0': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_0',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_0.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_0.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_0.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_0.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_0.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_0.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_1': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_1',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_1.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_1.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_1.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_1.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_1.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_1.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_2': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_2',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_2.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_2.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_2.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_2.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_2.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_2.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_3': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_3',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_3.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_3.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_3.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_3.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_3.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_3.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_4': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_4',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_4.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_4.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_4.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_4.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_4.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_4.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_5': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_5',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_5.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_5.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_5.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_5.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_5.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_5.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_6': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_6',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_6.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_6.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_6.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_6.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_6.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_6.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_7': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_7',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_7.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_7.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_7.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_7.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_7.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_7.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_8': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_8',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_8.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_8.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_8.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_8.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_8.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_8.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_9': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_9',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_9.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_9.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_9.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_9.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_9.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_9.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_10': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_10',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_10.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_10.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_10.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_10.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_10.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_10.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_11': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_11',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_11.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_11.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_11.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_11.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_11.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_11.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_12': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_12',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_12.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_12.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_12.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_12.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_12.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_12.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_13': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_13',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_13.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_13.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_13.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_13.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_13.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_13.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_14': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_14',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_14.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_14.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_14.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_14.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_14.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_14.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "        'encoder_layer_15': {'type': 'EncoderBlock',\n",
       "         'layer_name': 'enc.encoder.layers.encoder_layer_15',\n",
       "         'config': {'training': False, 'num_heads': 16},\n",
       "         'ln_1': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_15.ln_1',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'self_attention': {'type': 'MultiheadAttention',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_15.self_attention',\n",
       "          'config': {'training': False,\n",
       "           'embed_dim': 16,\n",
       "           'kdim': 16,\n",
       "           'vdim': 16,\n",
       "           'num_heads': 16,\n",
       "           'dropout': 0.0,\n",
       "           'batch_first': True,\n",
       "           'head_dim': 1,\n",
       "           'bias_k': None,\n",
       "           'bias_v': None,\n",
       "           'add_zero_attn': False},\n",
       "          'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_15.self_attention.out_proj',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 16}}},\n",
       "         'dropout': {'type': 'Dropout',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_15.dropout',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         'ln_2': {'type': 'LayerNorm',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_15.ln_2',\n",
       "          'config': {'training': False,\n",
       "           'normalized_shape': [16],\n",
       "           'eps': 1e-06,\n",
       "           'elementwise_affine': True}},\n",
       "         'mlp': {'type': 'MLPBlock',\n",
       "          'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp',\n",
       "          'config': {'training': False},\n",
       "          '0': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.0',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 16,\n",
       "            'out_features': 3072}},\n",
       "          '1': {'type': 'GELU',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.1',\n",
       "           'config': {'training': False, 'approximate': 'none'}},\n",
       "          '2': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.2',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "          '3': {'type': 'Linear',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.3',\n",
       "           'config': {'training': False,\n",
       "            'in_features': 3072,\n",
       "            'out_features': 16}},\n",
       "          '4': {'type': 'Dropout',\n",
       "           'layer_name': 'enc.encoder.layers.encoder_layer_15.mlp.4',\n",
       "           'config': {'training': False, 'p': 0.0, 'inplace': False}}}}},\n",
       "       'ln': {'type': 'LayerNorm',\n",
       "        'layer_name': 'enc.encoder.ln',\n",
       "        'config': {'training': False,\n",
       "         'normalized_shape': [16],\n",
       "         'eps': 1e-06,\n",
       "         'elementwise_affine': True}}},\n",
       "      'heads': {'type': 'Identity',\n",
       "       'layer_name': 'enc.heads',\n",
       "       'config': {'training': False}}},\n",
       "     '2-dec': {'dec': {'type': 'PatchDecoder',\n",
       "       'layer_name': 'dec',\n",
       "       'config': {'training': False,\n",
       "        'image_size': 256,\n",
       "        'patch_size': 16,\n",
       "        'num_patches': 256,\n",
       "        'emb_dim': 16}},\n",
       "      'proj': {'type': 'Linear',\n",
       "       'layer_name': 'dec.proj',\n",
       "       'config': {'training': False,\n",
       "        'in_features': 16,\n",
       "        'out_features': 65536}},\n",
       "      'patch_to_img': {'type': 'Sequential',\n",
       "       'layer_name': 'dec.patch_to_img',\n",
       "       'config': {'training': False},\n",
       "       '0': {'type': 'PixelShuffle',\n",
       "        'layer_name': 'dec.patch_to_img.0',\n",
       "        'config': {'training': False, 'upscale_factor': 1}},\n",
       "       '1': {'type': 'ConvTranspose2d',\n",
       "        'layer_name': 'dec.patch_to_img.1',\n",
       "        'config': {'training': False,\n",
       "         'in_channels': 1,\n",
       "         'out_channels': 1,\n",
       "         'kernel_size': [3, 3],\n",
       "         'stride': [1, 1],\n",
       "         'padding': [1, 1],\n",
       "         'dilation': [1, 1],\n",
       "         'transposed': True,\n",
       "         'output_padding': [0, 0],\n",
       "         'groups': 1,\n",
       "         'padding_mode': 'zeros'}}}},\n",
       "     '3-mn': {'mn': {'type': 'Linear',\n",
       "       'layer_name': 'mn',\n",
       "       'config': {'training': False, 'in_features': 16, 'out_features': 16}}},\n",
       "     '4-var': {'var': {'type': 'Linear',\n",
       "       'layer_name': 'var',\n",
       "       'config': {'training': False, 'in_features': 16, 'out_features': 16}}},\n",
       "     '5-relu_1': {'relu_1': {'type': 'ReLU',\n",
       "       'layer_name': 'relu_1',\n",
       "       'config': {'training': False, 'inplace': False}}}},\n",
       "    'training': False,\n",
       "    'num_vae': 1,\n",
       "    'fix_channel': False,\n",
       "    'channel_list': None,\n",
       "    'model_type': '4DSTEM',\n",
       "    'embedding_size': 16,\n",
       "    'set_topK': False,\n",
       "    'topk': 2},\n",
       "   'encoder': {'layers': {'1-conv_proj': {'conv_proj': {'type': 'Conv2d',\n",
       "       'layer_name': 'conv_proj',\n",
       "       'config': {'training': False,\n",
       "        'in_channels': 1,\n",
       "        'out_channels': 16,\n",
       "        'kernel_size': [16, 16],\n",
       "        'stride': [16, 16],\n",
       "        'padding': [0, 0],\n",
       "        'dilation': [1, 1],\n",
       "        'transposed': False,\n",
       "        'output_padding': [0, 0],\n",
       "        'groups': 1,\n",
       "        'padding_mode': 'zeros'}}},\n",
       "     '2-encoder': {'encoder': {'type': 'Encoder',\n",
       "       'layer_name': 'encoder',\n",
       "       'config': {'training': False}},\n",
       "      'dropout': {'type': 'Dropout',\n",
       "       'layer_name': 'encoder.dropout',\n",
       "       'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "      'layers': {'type': 'Sequential',\n",
       "       'layer_name': 'encoder.layers',\n",
       "       'config': {'training': False},\n",
       "       'encoder_layer_0': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_0',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_0.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_0.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_0.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_0.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_0.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_0.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_0.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_0.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_0.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_0.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_0.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_1': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_1',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_1.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_1.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_1.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_1.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_1.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_1.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_1.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_1.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_1.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_1.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_1.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_2': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_2',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_2.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_2.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_2.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_2.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_2.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_2.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_2.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_2.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_2.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_2.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_2.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_3': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_3',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_3.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_3.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_3.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_3.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_3.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_3.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_3.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_3.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_3.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_3.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_3.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_4': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_4',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_4.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_4.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_4.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_4.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_4.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_4.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_4.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_4.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_4.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_4.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_4.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_5': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_5',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_5.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_5.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_5.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_5.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_5.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_5.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_5.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_5.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_5.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_5.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_5.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_6': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_6',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_6.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_6.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_6.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_6.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_6.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_6.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_6.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_6.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_6.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_6.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_6.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_7': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_7',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_7.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_7.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_7.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_7.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_7.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_7.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_7.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_7.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_7.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_7.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_7.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_8': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_8',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_8.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_8.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_8.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_8.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_8.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_8.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_8.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_8.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_8.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_8.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_8.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_9': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_9',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_9.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_9.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_9.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_9.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_9.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_9.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_9.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_9.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_9.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_9.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_9.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_10': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_10',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_10.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_10.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_10.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_10.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_10.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_10.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_10.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_10.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_10.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_10.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_10.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_11': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_11',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_11.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_11.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_11.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_11.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_11.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_11.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_11.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_11.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_11.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_11.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_11.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_12': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_12',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_12.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_12.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_12.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_12.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_12.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_12.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_12.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_12.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_12.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_12.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_12.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_13': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_13',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_13.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_13.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_13.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_13.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_13.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_13.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_13.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_13.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_13.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_13.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_13.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_14': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_14',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_14.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_14.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_14.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_14.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_14.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_14.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_14.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_14.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_14.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_14.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_14.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}},\n",
       "       'encoder_layer_15': {'type': 'EncoderBlock',\n",
       "        'layer_name': 'encoder.layers.encoder_layer_15',\n",
       "        'config': {'training': False, 'num_heads': 16},\n",
       "        'ln_1': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_15.ln_1',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'self_attention': {'type': 'MultiheadAttention',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_15.self_attention',\n",
       "         'config': {'training': False,\n",
       "          'embed_dim': 16,\n",
       "          'kdim': 16,\n",
       "          'vdim': 16,\n",
       "          'num_heads': 16,\n",
       "          'dropout': 0.0,\n",
       "          'batch_first': True,\n",
       "          'head_dim': 1,\n",
       "          'bias_k': None,\n",
       "          'bias_v': None,\n",
       "          'add_zero_attn': False},\n",
       "         'out_proj': {'type': 'NonDynamicallyQuantizableLinear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_15.self_attention.out_proj',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 16}}},\n",
       "        'dropout': {'type': 'Dropout',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_15.dropout',\n",
       "         'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "        'ln_2': {'type': 'LayerNorm',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_15.ln_2',\n",
       "         'config': {'training': False,\n",
       "          'normalized_shape': [16],\n",
       "          'eps': 1e-06,\n",
       "          'elementwise_affine': True}},\n",
       "        'mlp': {'type': 'MLPBlock',\n",
       "         'layer_name': 'encoder.layers.encoder_layer_15.mlp',\n",
       "         'config': {'training': False},\n",
       "         '0': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_15.mlp.0',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 16,\n",
       "           'out_features': 3072}},\n",
       "         '1': {'type': 'GELU',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_15.mlp.1',\n",
       "          'config': {'training': False, 'approximate': 'none'}},\n",
       "         '2': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_15.mlp.2',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}},\n",
       "         '3': {'type': 'Linear',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_15.mlp.3',\n",
       "          'config': {'training': False,\n",
       "           'in_features': 3072,\n",
       "           'out_features': 16}},\n",
       "         '4': {'type': 'Dropout',\n",
       "          'layer_name': 'encoder.layers.encoder_layer_15.mlp.4',\n",
       "          'config': {'training': False, 'p': 0.0, 'inplace': False}}}}},\n",
       "      'ln': {'type': 'LayerNorm',\n",
       "       'layer_name': 'encoder.ln',\n",
       "       'config': {'training': False,\n",
       "        'normalized_shape': [16],\n",
       "        'eps': 1e-06,\n",
       "        'elementwise_affine': True}}},\n",
       "     '3-heads': {'heads': {'type': 'Identity',\n",
       "       'layer_name': 'heads',\n",
       "       'config': {'training': False}}}},\n",
       "    'training': False,\n",
       "    'image_size': 256,\n",
       "    'patch_size': 16,\n",
       "    'hidden_dim': 16,\n",
       "    'mlp_dim': 3072,\n",
       "    'attention_dropout': 0.0,\n",
       "    'dropout': 0.0,\n",
       "    'num_classes': 1000,\n",
       "    'representation_size': None,\n",
       "    'seq_length': 257},\n",
       "   'decoder': {'layers': {'1-proj': {'proj': {'type': 'Linear',\n",
       "       'layer_name': 'proj',\n",
       "       'config': {'training': False,\n",
       "        'in_features': 16,\n",
       "        'out_features': 65536}}},\n",
       "     '2-patch_to_img': {'patch_to_img': {'type': 'Sequential',\n",
       "       'layer_name': 'patch_to_img',\n",
       "       'config': {'training': False}},\n",
       "      '0': {'type': 'PixelShuffle',\n",
       "       'layer_name': 'patch_to_img.0',\n",
       "       'config': {'training': False, 'upscale_factor': 1}},\n",
       "      '1': {'type': 'ConvTranspose2d',\n",
       "       'layer_name': 'patch_to_img.1',\n",
       "       'config': {'training': False,\n",
       "        'in_channels': 1,\n",
       "        'out_channels': 1,\n",
       "        'kernel_size': [3, 3],\n",
       "        'stride': [1, 1],\n",
       "        'padding': [1, 1],\n",
       "        'dilation': [1, 1],\n",
       "        'transposed': True,\n",
       "        'output_padding': [0, 0],\n",
       "        'groups': 1,\n",
       "        'padding_mode': 'zeros'}}}},\n",
       "    'training': False,\n",
       "    'image_size': 256,\n",
       "    'patch_size': 16,\n",
       "    'num_patches': 256,\n",
       "    'emb_dim': 16},\n",
       "   'optimizer': {'lr': 0.0003,\n",
       "    'betas': [0.9, 0.999],\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0,\n",
       "    'amsgrad': False,\n",
       "    'maximize': False,\n",
       "    'foreach': None,\n",
       "    'capturable': False,\n",
       "    'differentiable': False,\n",
       "    'fused': None,\n",
       "    'params': [0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4,\n",
       "     5,\n",
       "     6,\n",
       "     7,\n",
       "     8,\n",
       "     9,\n",
       "     10,\n",
       "     11,\n",
       "     12,\n",
       "     13,\n",
       "     14,\n",
       "     15,\n",
       "     16,\n",
       "     17,\n",
       "     18,\n",
       "     19,\n",
       "     20,\n",
       "     21,\n",
       "     22,\n",
       "     23,\n",
       "     24,\n",
       "     25,\n",
       "     26,\n",
       "     27,\n",
       "     28,\n",
       "     29,\n",
       "     30,\n",
       "     31,\n",
       "     32,\n",
       "     33,\n",
       "     34,\n",
       "     35,\n",
       "     36,\n",
       "     37,\n",
       "     38,\n",
       "     39,\n",
       "     40,\n",
       "     41,\n",
       "     42,\n",
       "     43,\n",
       "     44,\n",
       "     45,\n",
       "     46,\n",
       "     47,\n",
       "     48,\n",
       "     49,\n",
       "     50,\n",
       "     51,\n",
       "     52,\n",
       "     53,\n",
       "     54,\n",
       "     55,\n",
       "     56,\n",
       "     57,\n",
       "     58,\n",
       "     59,\n",
       "     60,\n",
       "     61,\n",
       "     62,\n",
       "     63,\n",
       "     64,\n",
       "     65,\n",
       "     66,\n",
       "     67,\n",
       "     68,\n",
       "     69,\n",
       "     70,\n",
       "     71,\n",
       "     72,\n",
       "     73,\n",
       "     74,\n",
       "     75,\n",
       "     76,\n",
       "     77,\n",
       "     78,\n",
       "     79,\n",
       "     80,\n",
       "     81,\n",
       "     82,\n",
       "     83,\n",
       "     84,\n",
       "     85,\n",
       "     86,\n",
       "     87,\n",
       "     88,\n",
       "     89,\n",
       "     90,\n",
       "     91,\n",
       "     92,\n",
       "     93,\n",
       "     94,\n",
       "     95,\n",
       "     96,\n",
       "     97,\n",
       "     98,\n",
       "     99,\n",
       "     100,\n",
       "     101,\n",
       "     102,\n",
       "     103,\n",
       "     104,\n",
       "     105,\n",
       "     106,\n",
       "     107,\n",
       "     108,\n",
       "     109,\n",
       "     110,\n",
       "     111,\n",
       "     112,\n",
       "     113,\n",
       "     114,\n",
       "     115,\n",
       "     116,\n",
       "     117,\n",
       "     118,\n",
       "     119,\n",
       "     120,\n",
       "     121,\n",
       "     122,\n",
       "     123,\n",
       "     124,\n",
       "     125,\n",
       "     126,\n",
       "     127,\n",
       "     128,\n",
       "     129,\n",
       "     130,\n",
       "     131,\n",
       "     132,\n",
       "     133,\n",
       "     134,\n",
       "     135,\n",
       "     136,\n",
       "     137,\n",
       "     138,\n",
       "     139,\n",
       "     140,\n",
       "     141,\n",
       "     142,\n",
       "     143,\n",
       "     144,\n",
       "     145,\n",
       "     146,\n",
       "     147,\n",
       "     148,\n",
       "     149,\n",
       "     150,\n",
       "     151,\n",
       "     152,\n",
       "     153,\n",
       "     154,\n",
       "     155,\n",
       "     156,\n",
       "     157,\n",
       "     158,\n",
       "     159,\n",
       "     160,\n",
       "     161,\n",
       "     162,\n",
       "     163,\n",
       "     164,\n",
       "     165,\n",
       "     166,\n",
       "     167,\n",
       "     168,\n",
       "     169,\n",
       "     170,\n",
       "     171,\n",
       "     172,\n",
       "     173,\n",
       "     174,\n",
       "     175,\n",
       "     176,\n",
       "     177,\n",
       "     178,\n",
       "     179,\n",
       "     180,\n",
       "     181,\n",
       "     182,\n",
       "     183,\n",
       "     184,\n",
       "     185,\n",
       "     186,\n",
       "     187,\n",
       "     188,\n",
       "     189,\n",
       "     190,\n",
       "     191,\n",
       "     192,\n",
       "     193,\n",
       "     194,\n",
       "     195,\n",
       "     196,\n",
       "     197,\n",
       "     198,\n",
       "     199,\n",
       "     200,\n",
       "     201,\n",
       "     202,\n",
       "     203,\n",
       "     204,\n",
       "     205]}},\n",
       "  'folder_model': 'Transformer_VAE/4DSTEM/model_102424',\n",
       "  'folder_figure': 'Transformer_VAE/4DSTEM/fig_102424',\n",
       "  'N_EPOCHS': 62000,\n",
       "  'initial_epoch': 50,\n",
       "  'batch_size': 64,\n",
       "  'loadWeights': False,\n",
       "  'model_type': '4DSTEM',\n",
       "  'device': 'cuda:0',\n",
       "  'logging': True,\n",
       "  'log_folder': 'logs/Upload_to_Datafed/Transformer_VAE/4DSTEM',\n",
       "  'log_file': 'model_102424.txt',\n",
       "  'log_file_path': 'logs/Upload_to_Datafed/Transformer_VAE/4DSTEM/model_102424.txt',\n",
       "  'seed': 42,\n",
       "  'start_epoch': 0,\n",
       "  'num_vae': 1,\n",
       "  'model_architecture_names': ['vae', 'encoder', 'decoder', 'optimizer'],\n",
       "  'ref_mn_value': 0.1,\n",
       "  'ref_var_value': 0.01,\n",
       "  'epoch': 780,\n",
       "  'embedding_L1_norm': 0.000461019721115008,\n",
       "  'weights_file_path': 'Transformer_VAE/4DSTEM/model_102424/epoch_00780_train_loss_0.23238_KL1_0.00095_KL2_0.00000_L1_0.00046_contras_loss_0.00087_maxi_loss_0.00011_.pkl',\n",
       "  'image_size': [256, 256],\n",
       "  'embedding_file_path': 'Transformer_VAE/4DSTEM/fig_102424/epoch_00780_mse_0.22996_.png',\n",
       "  'zip_file_path': 'Transformer_VAE/4DSTEM/model_102424/Epoch_779_train_loss_2.3237e-01.zip',\n",
       "  'tb': 'Traceback (most recent call last):\\n  File \"/home/jg3837/Transformer_Beta_VAE/src/transformer_beta_vae/Transformer_VAE_train.py\", line 647, in Train\\n    torchlogger.save(f\"Epoch_{epoch}_train_loss_{format(train_loss,\\'.4e\\')}.zip\", epoch=epoch, training_loss=train_loss,\\n  File \"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src/datafed_torchflow/pytorch.py\", line 540, in save\\n    dc_resp = self.df_api.data_record_create(\\n  File \"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src/datafed_torchflow/datafed.py\", line 521, in data_record_create\\n    raise e\\n  File \"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src/datafed_torchflow/datafed.py\", line 497, in data_record_create\\n    dc_resp = self.dataCreate(\\n  File \"/home/jg3837/anaconda3/envs/TransformerVAE6/lib/python3.10/site-packages/datafed/CommandLib.py\", line 495, in dataCreate\\n    return self._mapi.sendRecv(msg)\\n  File \"/home/jg3837/anaconda3/envs/TransformerVAE6/lib/python3.10/site-packages/datafed/MessageLib.py\", line 389, in sendRecv\\n    raise Exception(\\nException: Mismatched reply. Expected 718 got 717\\n',\n",
       "  'current_user': 'jg3837',\n",
       "  'current_time': '2024-10-28 15:35:32',\n",
       "  'computer_info': {'cpu': {'physical_cores': 32,\n",
       "    'total_cores': 32,\n",
       "    'cpu_frequency': {'current': 3557.04875, 'min': 1500.0, 'max': 3000.0},\n",
       "    'cpu_usage_per_core': [63.9,\n",
       "     62.2,\n",
       "     63.9,\n",
       "     64.9,\n",
       "     62.9,\n",
       "     86.5,\n",
       "     63.2,\n",
       "     62.2,\n",
       "     62.9,\n",
       "     62.2,\n",
       "     62.2,\n",
       "     61.1,\n",
       "     63.9,\n",
       "     61.1,\n",
       "     62.2,\n",
       "     61.1,\n",
       "     63.9,\n",
       "     19.4,\n",
       "     63.2,\n",
       "     64.9,\n",
       "     35.1,\n",
       "     69.4,\n",
       "     62.9,\n",
       "     70.3,\n",
       "     61.1,\n",
       "     63.9,\n",
       "     75.7,\n",
       "     63.9,\n",
       "     52.8,\n",
       "     60.0,\n",
       "     42.9,\n",
       "     77.8],\n",
       "    'total_cpu_usage': 61.8},\n",
       "   'memory': {'total': '1007.68 GB',\n",
       "    'available': '539.22 GB',\n",
       "    'used': '462.99 GB',\n",
       "    'percent': 46.5},\n",
       "   'gpu': [{'id': 0,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '22025.00 MB',\n",
       "     'memory_free': '26646.00 MB',\n",
       "     'load': '0.00%',\n",
       "     'temperature': '60.0 °C'},\n",
       "    {'id': 1,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '12822.00 MB',\n",
       "     'memory_free': '35848.00 MB',\n",
       "     'load': '52.00%',\n",
       "     'temperature': '66.0 °C'},\n",
       "    {'id': 2,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '2276.00 MB',\n",
       "     'memory_free': '46394.00 MB',\n",
       "     'load': '0.00%',\n",
       "     'temperature': '29.0 °C'},\n",
       "    {'id': 3,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '13.00 MB',\n",
       "     'memory_free': '48657.00 MB',\n",
       "     'load': '0.00%',\n",
       "     'temperature': '28.0 °C'},\n",
       "    {'id': 4,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '278.00 MB',\n",
       "     'memory_free': '48392.00 MB',\n",
       "     'load': '0.00%',\n",
       "     'temperature': '28.0 °C'},\n",
       "    {'id': 5,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '388.00 MB',\n",
       "     'memory_free': '48282.00 MB',\n",
       "     'load': '0.00%',\n",
       "     'temperature': '28.0 °C'},\n",
       "    {'id': 6,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '13.00 MB',\n",
       "     'memory_free': '48657.00 MB',\n",
       "     'load': '0.00%',\n",
       "     'temperature': '28.0 °C'},\n",
       "    {'id': 7,\n",
       "     'name': 'NVIDIA RTX A6000',\n",
       "     'driver_version': '550.107.02',\n",
       "     'memory_total': '49140.00 MB',\n",
       "     'memory_used': '13.00 MB',\n",
       "     'memory_free': '48657.00 MB',\n",
       "     'load': '0.00%',\n",
       "     'temperature': '29.0 °C'}],\n",
       "   'python': {'python_version': '3.10.12',\n",
       "    'python_implementation': 'CPython',\n",
       "    'python_build': ['main', 'Jul  5 2023 18:54:27'],\n",
       "    'packages': {'zipp': '3.19.2',\n",
       "     'zict': '3.0.0',\n",
       "     'zenodopy': '0.3.0',\n",
       "     'yarl': '1.13.1',\n",
       "     'xrayutilities': '1.7.8',\n",
       "     'xlrd': '2.0.1',\n",
       "     'wrapt': '1.16.0',\n",
       "     'widgetsnbextension': '4.0.11',\n",
       "     'wheel': '0.43.0',\n",
       "     'wget': '3.2',\n",
       "     'werkzeug': '3.0.3',\n",
       "     'websocket-client': '1.8.0',\n",
       "     'webencodings': '0.5.1',\n",
       "     'webcolors': '24.6.0',\n",
       "     'wcwidth': '0.2.13',\n",
       "     'urllib3': '1.26.20',\n",
       "     'uri-template': '1.3.0',\n",
       "     'uncertainties': '3.2.2',\n",
       "     'uc-micro-py': '1.0.3',\n",
       "     'tzdata': '2024.1',\n",
       "     'typing-inspect': '0.9.0',\n",
       "     'typing-extensions': '4.12.2',\n",
       "     'types-requests': '2.32.0.20240712',\n",
       "     'types-python-dateutil': '2.9.0.20240316',\n",
       "     'typeguard': '4.3.0',\n",
       "     'triton': '3.0.0',\n",
       "     'traits': '6.4.3',\n",
       "     'traitlets': '5.14.3',\n",
       "     'tqdm': '4.66.4',\n",
       "     'tornado': '6.4.1',\n",
       "     'torchx': '0.7.0',\n",
       "     'torchvision': '0.19.0',\n",
       "     'torchsummary': '1.5.1',\n",
       "     'torchprofile': '0.0.4',\n",
       "     'torchmetrics': '1.4.2',\n",
       "     'torchaudio': '2.4.0',\n",
       "     'torch': '2.4.0',\n",
       "     'toolz': '0.12.1',\n",
       "     'tomli': '2.0.1',\n",
       "     'tinycss2': '1.3.0',\n",
       "     'tifffile': '2024.7.24',\n",
       "     'threadpoolctl': '3.5.0',\n",
       "     'terminado': '0.18.1',\n",
       "     'termcolor': '2.4.0',\n",
       "     'tensorly': '0.8.1',\n",
       "     'tensorflow': '2.17.0',\n",
       "     'tensorflow-io-gcs-filesystem': '0.37.1',\n",
       "     'tensorboard': '2.17.0',\n",
       "     'tensorboard-data-server': '0.7.2',\n",
       "     'tenacity': '9.0.0',\n",
       "     'tblib': '3.0.0',\n",
       "     'tabulate': '0.9.0',\n",
       "     'sympy': '1.13.1',\n",
       "     'stack-data': '0.6.3',\n",
       "     'sqlalchemy': '2.0.31',\n",
       "     'sphinxcontrib-serializinghtml': '2.0.0',\n",
       "     'sphinxcontrib-qthelp': '2.0.0',\n",
       "     'sphinxcontrib-jsmath': '1.0.1',\n",
       "     'sphinxcontrib-htmlhelp': '2.1.0',\n",
       "     'sphinxcontrib-devhelp': '2.0.0',\n",
       "     'sphinxcontrib-bibtex': '2.6.2',\n",
       "     'sphinxcontrib-applehelp': '2.0.0',\n",
       "     'sphinx': '7.4.7',\n",
       "     'sphinx-togglebutton': '0.3.2',\n",
       "     'sphinx-thebe': '0.3.1',\n",
       "     'sphinx-multitoc-numbering': '0.1.3',\n",
       "     'sphinx-jupyterbook-latex': '1.0.0',\n",
       "     'sphinx-external-toc': '1.0.1',\n",
       "     'sphinx-design': '0.6.0',\n",
       "     'sphinx-copybutton': '0.5.2',\n",
       "     'sphinx-comments': '0.0.3',\n",
       "     'sphinx-book-theme': '1.1.3',\n",
       "     'spglib': '2.5.0',\n",
       "     'soupsieve': '2.5',\n",
       "     'sortedcontainers': '2.4.0',\n",
       "     'snowballstemmer': '2.2.0',\n",
       "     'sniffio': '1.3.1',\n",
       "     'six': '1.16.0',\n",
       "     'simpy': '4.1.1',\n",
       "     'simpleitk': '2.3.1',\n",
       "     'sidpy': '0.12.3',\n",
       "     'setuptools': '58.2.0',\n",
       "     'send2trash': '1.8.3',\n",
       "     'seaborn': '0.13.2',\n",
       "     'scipy': '1.10.1',\n",
       "     'scikit-learn': '1.5.1',\n",
       "     'scikit-image': '0.20.0',\n",
       "     'scifireaders': '0.11.5',\n",
       "     'rpds-py': '0.19.1',\n",
       "     'rosettasciio': '0.6',\n",
       "     'rich': '13.7.1',\n",
       "     'rfc3986-validator': '0.1.1',\n",
       "     'rfc3339-validator': '0.1.4',\n",
       "     'requests': '2.32.3',\n",
       "     'referencing': '0.35.1',\n",
       "     'qtpy': '2.4.1',\n",
       "     'qtconsole': '5.5.2',\n",
       "     'pyzmq': '26.0.3',\n",
       "     'pyyaml': '6.0.1',\n",
       "     'pywget': '0.31',\n",
       "     'pywavelets': '1.6.0',\n",
       "     'pyusid': '0.0.12',\n",
       "     'pytz': '2024.1',\n",
       "     'pytorch-lightning': '2.4.0',\n",
       "     'python-json-logger': '2.0.7',\n",
       "     'python-dateutil': '2.9.0.post0',\n",
       "     'python-box': '7.2.0',\n",
       "     'pytest': '8.3.3',\n",
       "     'pytemlib': '0.2024.2.2',\n",
       "     'pysptools': '0.15.0',\n",
       "     'pysocks': '1.7.1',\n",
       "     'pyro-ppl': '1.9.1',\n",
       "     'pyro-api': '0.1.2',\n",
       "     'pyre-extensions': '0.0.31',\n",
       "     'pyparsing': '3.1.2',\n",
       "     'pynsid': '0.0.7.2',\n",
       "     'pyjwt': '2.9.0',\n",
       "     'pygments': '2.18.0',\n",
       "     'pydata-sphinx-theme': '0.15.4',\n",
       "     'pydantic': '2.9.2',\n",
       "     'pydantic-core': '2.23.4',\n",
       "     'pycroscopy': '0.63.3',\n",
       "     'pycparser': '2.22',\n",
       "     'pycodestyle': '2.12.0',\n",
       "     'pybtex': '0.24.0',\n",
       "     'pybtex-docutils': '1.0.3',\n",
       "     'py-cpuinfo': '9.0.0',\n",
       "     'pure-eval': '0.2.3',\n",
       "     'ptyprocess': '0.7.0',\n",
       "     'ptflops': '0.7.4',\n",
       "     'psutil': '6.0.0',\n",
       "     'protobuf': '4.25.5',\n",
       "     'prompt-toolkit': '3.0.47',\n",
       "     'prometheus-client': '0.20.0',\n",
       "     'prettytable': '3.10.2',\n",
       "     'pooch': '1.8.2',\n",
       "     'pluggy': '1.5.0',\n",
       "     'plotly': '5.23.0',\n",
       "     'platformdirs': '4.2.2',\n",
       "     'pip': '24.0',\n",
       "     'pint': '0.24.3',\n",
       "     'pillow': '10.4.0',\n",
       "     'pexpect': '4.9.0',\n",
       "     'partd': '1.4.2',\n",
       "     'parso': '0.8.4',\n",
       "     'pandocfilters': '1.5.1',\n",
       "     'pandas': '2.2.2',\n",
       "     'packaging': '24.1',\n",
       "     'overrides': '7.7.0',\n",
       "     'optree': '0.12.1',\n",
       "     'opt-einsum': '3.3.0',\n",
       "     'opencv-python': '4.10.0.84',\n",
       "     'nvidia-nvtx-cu12': '12.1.105',\n",
       "     'nvidia-nvjitlink-cu12': '12.5.82',\n",
       "     'nvidia-nccl-cu12': '2.20.5',\n",
       "     'nvidia-ml-py': '12.560.30',\n",
       "     'nvidia-cusparse-cu12': '12.1.0.106',\n",
       "     'nvidia-cusolver-cu12': '11.4.5.107',\n",
       "     'nvidia-curand-cu12': '10.3.2.106',\n",
       "     'nvidia-cufft-cu12': '11.0.2.54',\n",
       "     'nvidia-cudnn-cu12': '9.1.0.70',\n",
       "     'nvidia-cuda-runtime-cu12': '12.1.105',\n",
       "     'nvidia-cuda-nvrtc-cu12': '12.1.105',\n",
       "     'nvidia-cuda-cupti-cu12': '12.1.105',\n",
       "     'nvidia-cublas-cu12': '12.1.3.1',\n",
       "     'numpy': '1.26.4',\n",
       "     'numpy-groupies': '0.9.7',\n",
       "     'numba': '0.60.0',\n",
       "     'notebook': '7.2.1',\n",
       "     'notebook-shim': '0.2.4',\n",
       "     'ninja': '1.11.1.1',\n",
       "     'networkx': '3.3',\n",
       "     'nest-asyncio': '1.6.0',\n",
       "     'nbformat': '5.10.4',\n",
       "     'nbconvert': '7.16.4',\n",
       "     'nbclient': '0.10.0',\n",
       "     'natsort': '8.4.0',\n",
       "     'namex': '0.0.8',\n",
       "     'myst-parser': '2.0.0',\n",
       "     'myst-nb': '1.1.1',\n",
       "     'mypy-extensions': '1.0.0',\n",
       "     'multipledispatch': '1.0.0',\n",
       "     'multidict': '6.1.0',\n",
       "     'msgpack': '1.0.8',\n",
       "     'mpmath': '1.3.0',\n",
       "     'ml-dtypes': '0.4.0',\n",
       "     'mkl-service': '2.4.0',\n",
       "     'mkl-random': '1.2.4',\n",
       "     'mkl-fft': '1.3.8',\n",
       "     'mistune': '3.0.2',\n",
       "     'mdurl': '0.1.2',\n",
       "     'mdit-py-plugins': '0.4.1',\n",
       "     'matplotlib': '3.9.1',\n",
       "     'matplotlib-inline': '0.1.7',\n",
       "     'markupsafe': '2.1.5',\n",
       "     'markdown': '3.6',\n",
       "     'markdown-it-py': '3.0.0',\n",
       "     'm3learning-util': '2.0.3',\n",
       "     'm3-learning': '0.0.24',\n",
       "     'lxml': '5.2.2',\n",
       "     'locket': '1.0.0',\n",
       "     'lmfit': '1.3.2',\n",
       "     'llvmlite': '0.43.0',\n",
       "     'linkify-it-py': '2.0.3',\n",
       "     'linear-operator': '0.5.3',\n",
       "     'lightning-utilities': '0.11.7',\n",
       "     'libclang': '18.1.1',\n",
       "     'lazy-loader': '0.4',\n",
       "     'latexcodec': '3.0.0',\n",
       "     'kiwisolver': '1.4.5',\n",
       "     'keras': '3.4.1',\n",
       "     'jupyterlab': '4.2.4',\n",
       "     'jupyterlab-widgets': '3.0.11',\n",
       "     'jupyterlab-server': '2.27.3',\n",
       "     'jupyterlab-pygments': '0.3.0',\n",
       "     'jupyter': '1.0.0',\n",
       "     'jupyter-server': '2.14.2',\n",
       "     'jupyter-server-terminals': '0.5.3',\n",
       "     'jupyter-lsp': '2.2.5',\n",
       "     'jupyter-events': '0.10.0',\n",
       "     'jupyter-core': '5.7.2',\n",
       "     'jupyter-console': '6.6.3',\n",
       "     'jupyter-client': '8.6.2',\n",
       "     'jupyter-cache': '1.0.0',\n",
       "     'jupyter-book': '1.0.2',\n",
       "     'jsonschema': '4.23.0',\n",
       "     'jsonschema-specifications': '2023.12.1',\n",
       "     'jsonpointer': '3.0.0',\n",
       "     'jsonpickle': '3.3.0',\n",
       "     'json5': '0.9.25',\n",
       "     'joblib': '1.4.2',\n",
       "     'jmespath': '1.0.1',\n",
       "     'jinja2': '3.1.4',\n",
       "     'jedi': '0.19.1',\n",
       "     'jaxtyping': '0.2.19',\n",
       "     'isoduration': '20.11.0',\n",
       "     'ipywidgets': '8.1.3',\n",
       "     'ipython': '8.26.0',\n",
       "     'ipython-genutils': '0.2.0',\n",
       "     'ipympl': '0.9.4',\n",
       "     'ipykernel': '6.29.5',\n",
       "     'iniconfig': '2.0.0',\n",
       "     'importlib-metadata': '8.2.0',\n",
       "     'imagesize': '1.4.1',\n",
       "     'imageio': '2.22.3',\n",
       "     'idna': '3.7',\n",
       "     'hyperspy': '2.1.1',\n",
       "     'httpx': '0.27.0',\n",
       "     'httpcore': '1.0.5',\n",
       "     'hjson': '3.1.0',\n",
       "     'h11': '0.14.0',\n",
       "     'h5py': '3.11.0',\n",
       "     'grpcio': '1.65.2',\n",
       "     'greenlet': '3.0.3',\n",
       "     'gpytorch': '1.13',\n",
       "     'gputil': '1.4.0',\n",
       "     'google-pasta': '0.2.0',\n",
       "     'gmpy2': '2.1.2',\n",
       "     'globus-sdk': '3.44.0',\n",
       "     'globus-cli': '3.30.1',\n",
       "     'gast': '0.6.0',\n",
       "     'fsspec': '2024.6.1',\n",
       "     'frozenlist': '1.4.1',\n",
       "     'fqdn': '1.5.1',\n",
       "     'fonttools': '4.53.1',\n",
       "     'flexparser': '0.3.1',\n",
       "     'flexcache': '0.3',\n",
       "     'flatbuffers': '24.3.25',\n",
       "     'filelock': '3.15.4',\n",
       "     'fastjsonschema': '2.20.0',\n",
       "     'executing': '2.0.1',\n",
       "     'exceptiongroup': '1.2.2',\n",
       "     'docutils': '0.20.1',\n",
       "     'docstring-parser': '0.16',\n",
       "     'docker': '7.1.0',\n",
       "     'distributed': '2024.7.1',\n",
       "     'dill': '0.3.8',\n",
       "     'defusedxml': '0.7.1',\n",
       "     'deepspeed': '0.15.2',\n",
       "     'decorator': '5.1.1',\n",
       "     'debugpy': '1.8.2',\n",
       "     'datafed': '3.0.0',\n",
       "     'dask': '2024.7.1',\n",
       "     'cytoolz': '0.12.3',\n",
       "     'cycler': '0.12.1',\n",
       "     'cvxopt': '1.3.2',\n",
       "     'cryptography': '43.0.1',\n",
       "     'contourpy': '1.2.1',\n",
       "     'comm': '0.2.2',\n",
       "     'cloudpickle': '3.0.0',\n",
       "     'click': '8.1.7',\n",
       "     'charset-normalizer': '3.3.2',\n",
       "     'cffi': '1.16.0',\n",
       "     'certifi': '2024.7.4',\n",
       "     'brotli': '1.0.9',\n",
       "     'botorch': '0.12.0',\n",
       "     'bleach': '6.1.0',\n",
       "     'bglib': '0.0.4',\n",
       "     'beautifulsoup4': '4.12.3',\n",
       "     'babel': '2.15.0',\n",
       "     'ax-platform': '0.4.3',\n",
       "     'autophyslearn': '0.2.5',\n",
       "     'autopep8': '2.3.1',\n",
       "     'attrs': '23.2.0',\n",
       "     'async-timeout': '4.0.3',\n",
       "     'async-lru': '2.0.4',\n",
       "     'astunparse': '1.6.3',\n",
       "     'asttokens': '2.4.1',\n",
       "     'asteval': '1.0.1',\n",
       "     'ase': '3.23.0',\n",
       "     'arrow': '1.3.0',\n",
       "     'argon2-cffi': '23.1.0',\n",
       "     'argon2-cffi-bindings': '21.2.0',\n",
       "     'appdirs': '1.4.4',\n",
       "     'anyio': '4.4.0',\n",
       "     'annotated-types': '0.7.0',\n",
       "     'alabaster': '0.7.16',\n",
       "     'aiosignal': '1.3.1',\n",
       "     'aiohttp': '3.10.8',\n",
       "     'aiohappyeyeballs': '2.4.3',\n",
       "     'accessible-pygments': '0.0.5',\n",
       "     'absl-py': '2.1.0'}}},\n",
       "  'metadata_file_folder': 'Transformer_VAE/4DSTEM/metadata_files',\n",
       "  'metadata_file': 'Epoch_779.json',\n",
       "  'metadata_file_path': 'Transformer_VAE/4DSTEM/metadata_files/Epoch_779.json',\n",
       "  'outfile': {'mode': 'w'},\n",
       "  'script': {'path': '/home/jg3837/Transformer_Beta_VAE/src/transformer_beta_vae/Transformer_VAE.ipynb',\n",
       "   'checksum': '26f7d2a62b7968115589217341980b04d2d8f962eecc878b07b943f624c6e4a2'},\n",
       "  'user': 'jg3837',\n",
       "  'timestamp': '2024-10-28 15:42:03'},\n",
       " 'System Information': {'cpu': {'physical_cores': 32,\n",
       "   'total_cores': 32,\n",
       "   'cpu_frequency': {'current': 1713.01134375, 'min': 1500.0, 'max': 3000.0},\n",
       "   'cpu_usage_per_core': [0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    2.2,\n",
       "    100.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    2.2,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    2.3,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    60.0,\n",
       "    0.0,\n",
       "    11.4,\n",
       "    0.0,\n",
       "    2.2,\n",
       "    0.0,\n",
       "    0.0],\n",
       "   'total_cpu_usage': 5.8},\n",
       "  'memory': {'total': '1007.68 GB',\n",
       "   'available': '538.85 GB',\n",
       "   'used': '463.35 GB',\n",
       "   'percent': 46.5},\n",
       "  'gpu': [{'id': 0,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '22025.00 MB',\n",
       "    'memory_free': '26646.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '61.0 °C'},\n",
       "   {'id': 1,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '12822.00 MB',\n",
       "    'memory_free': '35848.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '68.0 °C'},\n",
       "   {'id': 2,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '2276.00 MB',\n",
       "    'memory_free': '46394.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '29.0 °C'},\n",
       "   {'id': 3,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '13.00 MB',\n",
       "    'memory_free': '48657.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '28.0 °C'},\n",
       "   {'id': 4,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '278.00 MB',\n",
       "    'memory_free': '48392.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '28.0 °C'},\n",
       "   {'id': 5,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '388.00 MB',\n",
       "    'memory_free': '48282.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '28.0 °C'},\n",
       "   {'id': 6,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '13.00 MB',\n",
       "    'memory_free': '48657.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '28.0 °C'},\n",
       "   {'id': 7,\n",
       "    'name': 'NVIDIA RTX A6000',\n",
       "    'driver_version': '550.107.02',\n",
       "    'memory_total': '49140.00 MB',\n",
       "    'memory_used': '13.00 MB',\n",
       "    'memory_free': '48657.00 MB',\n",
       "    'load': '0.00%',\n",
       "    'temperature': '29.0 °C'}],\n",
       "  'python': {'python_version': '3.10.12',\n",
       "   'python_implementation': 'CPython',\n",
       "   'python_build': ['main', 'Jul  5 2023 18:54:27'],\n",
       "   'packages': {'zipp': '3.19.2',\n",
       "    'zict': '3.0.0',\n",
       "    'zenodopy': '0.3.0',\n",
       "    'yarl': '1.13.1',\n",
       "    'xrayutilities': '1.7.8',\n",
       "    'xlrd': '2.0.1',\n",
       "    'wrapt': '1.16.0',\n",
       "    'widgetsnbextension': '4.0.11',\n",
       "    'wheel': '0.43.0',\n",
       "    'wget': '3.2',\n",
       "    'werkzeug': '3.0.3',\n",
       "    'websocket-client': '1.8.0',\n",
       "    'webencodings': '0.5.1',\n",
       "    'webcolors': '24.6.0',\n",
       "    'wcwidth': '0.2.13',\n",
       "    'urllib3': '1.26.20',\n",
       "    'uri-template': '1.3.0',\n",
       "    'uncertainties': '3.2.2',\n",
       "    'uc-micro-py': '1.0.3',\n",
       "    'tzdata': '2024.1',\n",
       "    'typing-inspect': '0.9.0',\n",
       "    'typing-extensions': '4.12.2',\n",
       "    'types-requests': '2.32.0.20240712',\n",
       "    'types-python-dateutil': '2.9.0.20240316',\n",
       "    'typeguard': '4.3.0',\n",
       "    'triton': '3.0.0',\n",
       "    'traits': '6.4.3',\n",
       "    'traitlets': '5.14.3',\n",
       "    'tqdm': '4.66.4',\n",
       "    'tornado': '6.4.1',\n",
       "    'torchx': '0.7.0',\n",
       "    'torchvision': '0.19.0',\n",
       "    'torchsummary': '1.5.1',\n",
       "    'torchprofile': '0.0.4',\n",
       "    'torchmetrics': '1.4.2',\n",
       "    'torchaudio': '2.4.0',\n",
       "    'torch': '2.4.0',\n",
       "    'toolz': '0.12.1',\n",
       "    'tomli': '2.0.1',\n",
       "    'tinycss2': '1.3.0',\n",
       "    'tifffile': '2024.7.24',\n",
       "    'threadpoolctl': '3.5.0',\n",
       "    'terminado': '0.18.1',\n",
       "    'termcolor': '2.4.0',\n",
       "    'tensorly': '0.8.1',\n",
       "    'tensorflow': '2.17.0',\n",
       "    'tensorflow-io-gcs-filesystem': '0.37.1',\n",
       "    'tensorboard': '2.17.0',\n",
       "    'tensorboard-data-server': '0.7.2',\n",
       "    'tenacity': '9.0.0',\n",
       "    'tblib': '3.0.0',\n",
       "    'tabulate': '0.9.0',\n",
       "    'sympy': '1.13.1',\n",
       "    'stack-data': '0.6.3',\n",
       "    'sqlalchemy': '2.0.31',\n",
       "    'sphinxcontrib-serializinghtml': '2.0.0',\n",
       "    'sphinxcontrib-qthelp': '2.0.0',\n",
       "    'sphinxcontrib-jsmath': '1.0.1',\n",
       "    'sphinxcontrib-htmlhelp': '2.1.0',\n",
       "    'sphinxcontrib-devhelp': '2.0.0',\n",
       "    'sphinxcontrib-bibtex': '2.6.2',\n",
       "    'sphinxcontrib-applehelp': '2.0.0',\n",
       "    'sphinx': '7.4.7',\n",
       "    'sphinx-togglebutton': '0.3.2',\n",
       "    'sphinx-thebe': '0.3.1',\n",
       "    'sphinx-multitoc-numbering': '0.1.3',\n",
       "    'sphinx-jupyterbook-latex': '1.0.0',\n",
       "    'sphinx-external-toc': '1.0.1',\n",
       "    'sphinx-design': '0.6.0',\n",
       "    'sphinx-copybutton': '0.5.2',\n",
       "    'sphinx-comments': '0.0.3',\n",
       "    'sphinx-book-theme': '1.1.3',\n",
       "    'spglib': '2.5.0',\n",
       "    'soupsieve': '2.5',\n",
       "    'sortedcontainers': '2.4.0',\n",
       "    'snowballstemmer': '2.2.0',\n",
       "    'sniffio': '1.3.1',\n",
       "    'six': '1.16.0',\n",
       "    'simpy': '4.1.1',\n",
       "    'simpleitk': '2.3.1',\n",
       "    'sidpy': '0.12.3',\n",
       "    'setuptools': '58.2.0',\n",
       "    'send2trash': '1.8.3',\n",
       "    'seaborn': '0.13.2',\n",
       "    'scipy': '1.10.1',\n",
       "    'scikit-learn': '1.5.1',\n",
       "    'scikit-image': '0.20.0',\n",
       "    'scifireaders': '0.11.5',\n",
       "    'rpds-py': '0.19.1',\n",
       "    'rosettasciio': '0.6',\n",
       "    'rich': '13.7.1',\n",
       "    'rfc3986-validator': '0.1.1',\n",
       "    'rfc3339-validator': '0.1.4',\n",
       "    'requests': '2.32.3',\n",
       "    'referencing': '0.35.1',\n",
       "    'qtpy': '2.4.1',\n",
       "    'qtconsole': '5.5.2',\n",
       "    'pyzmq': '26.0.3',\n",
       "    'pyyaml': '6.0.1',\n",
       "    'pywget': '0.31',\n",
       "    'pywavelets': '1.6.0',\n",
       "    'pyusid': '0.0.12',\n",
       "    'pytz': '2024.1',\n",
       "    'pytorch-lightning': '2.4.0',\n",
       "    'python-json-logger': '2.0.7',\n",
       "    'python-dateutil': '2.9.0.post0',\n",
       "    'python-box': '7.2.0',\n",
       "    'pytest': '8.3.3',\n",
       "    'pytemlib': '0.2024.2.2',\n",
       "    'pysptools': '0.15.0',\n",
       "    'pysocks': '1.7.1',\n",
       "    'pyro-ppl': '1.9.1',\n",
       "    'pyro-api': '0.1.2',\n",
       "    'pyre-extensions': '0.0.31',\n",
       "    'pyparsing': '3.1.2',\n",
       "    'pynsid': '0.0.7.2',\n",
       "    'pyjwt': '2.9.0',\n",
       "    'pygments': '2.18.0',\n",
       "    'pydata-sphinx-theme': '0.15.4',\n",
       "    'pydantic': '2.9.2',\n",
       "    'pydantic-core': '2.23.4',\n",
       "    'pycroscopy': '0.63.3',\n",
       "    'pycparser': '2.22',\n",
       "    'pycodestyle': '2.12.0',\n",
       "    'pybtex': '0.24.0',\n",
       "    'pybtex-docutils': '1.0.3',\n",
       "    'py-cpuinfo': '9.0.0',\n",
       "    'pure-eval': '0.2.3',\n",
       "    'ptyprocess': '0.7.0',\n",
       "    'ptflops': '0.7.4',\n",
       "    'psutil': '6.0.0',\n",
       "    'protobuf': '4.25.5',\n",
       "    'prompt-toolkit': '3.0.47',\n",
       "    'prometheus-client': '0.20.0',\n",
       "    'prettytable': '3.10.2',\n",
       "    'pooch': '1.8.2',\n",
       "    'pluggy': '1.5.0',\n",
       "    'plotly': '5.23.0',\n",
       "    'platformdirs': '4.2.2',\n",
       "    'pip': '24.0',\n",
       "    'pint': '0.24.3',\n",
       "    'pillow': '10.4.0',\n",
       "    'pexpect': '4.9.0',\n",
       "    'partd': '1.4.2',\n",
       "    'parso': '0.8.4',\n",
       "    'pandocfilters': '1.5.1',\n",
       "    'pandas': '2.2.2',\n",
       "    'packaging': '24.1',\n",
       "    'overrides': '7.7.0',\n",
       "    'optree': '0.12.1',\n",
       "    'opt-einsum': '3.3.0',\n",
       "    'opencv-python': '4.10.0.84',\n",
       "    'nvidia-nvtx-cu12': '12.1.105',\n",
       "    'nvidia-nvjitlink-cu12': '12.5.82',\n",
       "    'nvidia-nccl-cu12': '2.20.5',\n",
       "    'nvidia-ml-py': '12.560.30',\n",
       "    'nvidia-cusparse-cu12': '12.1.0.106',\n",
       "    'nvidia-cusolver-cu12': '11.4.5.107',\n",
       "    'nvidia-curand-cu12': '10.3.2.106',\n",
       "    'nvidia-cufft-cu12': '11.0.2.54',\n",
       "    'nvidia-cudnn-cu12': '9.1.0.70',\n",
       "    'nvidia-cuda-runtime-cu12': '12.1.105',\n",
       "    'nvidia-cuda-nvrtc-cu12': '12.1.105',\n",
       "    'nvidia-cuda-cupti-cu12': '12.1.105',\n",
       "    'nvidia-cublas-cu12': '12.1.3.1',\n",
       "    'numpy': '1.26.4',\n",
       "    'numpy-groupies': '0.9.7',\n",
       "    'numba': '0.60.0',\n",
       "    'notebook': '7.2.1',\n",
       "    'notebook-shim': '0.2.4',\n",
       "    'ninja': '1.11.1.1',\n",
       "    'networkx': '3.3',\n",
       "    'nest-asyncio': '1.6.0',\n",
       "    'nbformat': '5.10.4',\n",
       "    'nbconvert': '7.16.4',\n",
       "    'nbclient': '0.10.0',\n",
       "    'natsort': '8.4.0',\n",
       "    'namex': '0.0.8',\n",
       "    'myst-parser': '2.0.0',\n",
       "    'myst-nb': '1.1.1',\n",
       "    'mypy-extensions': '1.0.0',\n",
       "    'multipledispatch': '1.0.0',\n",
       "    'multidict': '6.1.0',\n",
       "    'msgpack': '1.0.8',\n",
       "    'mpmath': '1.3.0',\n",
       "    'ml-dtypes': '0.4.0',\n",
       "    'mkl-service': '2.4.0',\n",
       "    'mkl-random': '1.2.4',\n",
       "    'mkl-fft': '1.3.8',\n",
       "    'mistune': '3.0.2',\n",
       "    'mdurl': '0.1.2',\n",
       "    'mdit-py-plugins': '0.4.1',\n",
       "    'matplotlib': '3.9.1',\n",
       "    'matplotlib-inline': '0.1.7',\n",
       "    'markupsafe': '2.1.5',\n",
       "    'markdown': '3.6',\n",
       "    'markdown-it-py': '3.0.0',\n",
       "    'm3learning-util': '2.0.3',\n",
       "    'm3-learning': '0.0.24',\n",
       "    'lxml': '5.2.2',\n",
       "    'locket': '1.0.0',\n",
       "    'lmfit': '1.3.2',\n",
       "    'llvmlite': '0.43.0',\n",
       "    'linkify-it-py': '2.0.3',\n",
       "    'linear-operator': '0.5.3',\n",
       "    'lightning-utilities': '0.11.7',\n",
       "    'libclang': '18.1.1',\n",
       "    'lazy-loader': '0.4',\n",
       "    'latexcodec': '3.0.0',\n",
       "    'kiwisolver': '1.4.5',\n",
       "    'keras': '3.4.1',\n",
       "    'jupyterlab': '4.2.4',\n",
       "    'jupyterlab-widgets': '3.0.11',\n",
       "    'jupyterlab-server': '2.27.3',\n",
       "    'jupyterlab-pygments': '0.3.0',\n",
       "    'jupyter': '1.0.0',\n",
       "    'jupyter-server': '2.14.2',\n",
       "    'jupyter-server-terminals': '0.5.3',\n",
       "    'jupyter-lsp': '2.2.5',\n",
       "    'jupyter-events': '0.10.0',\n",
       "    'jupyter-core': '5.7.2',\n",
       "    'jupyter-console': '6.6.3',\n",
       "    'jupyter-client': '8.6.2',\n",
       "    'jupyter-cache': '1.0.0',\n",
       "    'jupyter-book': '1.0.2',\n",
       "    'jsonschema': '4.23.0',\n",
       "    'jsonschema-specifications': '2023.12.1',\n",
       "    'jsonpointer': '3.0.0',\n",
       "    'jsonpickle': '3.3.0',\n",
       "    'json5': '0.9.25',\n",
       "    'joblib': '1.4.2',\n",
       "    'jmespath': '1.0.1',\n",
       "    'jinja2': '3.1.4',\n",
       "    'jedi': '0.19.1',\n",
       "    'jaxtyping': '0.2.19',\n",
       "    'isoduration': '20.11.0',\n",
       "    'ipywidgets': '8.1.3',\n",
       "    'ipython': '8.26.0',\n",
       "    'ipython-genutils': '0.2.0',\n",
       "    'ipympl': '0.9.4',\n",
       "    'ipykernel': '6.29.5',\n",
       "    'iniconfig': '2.0.0',\n",
       "    'importlib-metadata': '8.2.0',\n",
       "    'imagesize': '1.4.1',\n",
       "    'imageio': '2.22.3',\n",
       "    'idna': '3.7',\n",
       "    'hyperspy': '2.1.1',\n",
       "    'httpx': '0.27.0',\n",
       "    'httpcore': '1.0.5',\n",
       "    'hjson': '3.1.0',\n",
       "    'h11': '0.14.0',\n",
       "    'h5py': '3.11.0',\n",
       "    'grpcio': '1.65.2',\n",
       "    'greenlet': '3.0.3',\n",
       "    'gpytorch': '1.13',\n",
       "    'gputil': '1.4.0',\n",
       "    'google-pasta': '0.2.0',\n",
       "    'gmpy2': '2.1.2',\n",
       "    'globus-sdk': '3.44.0',\n",
       "    'globus-cli': '3.30.1',\n",
       "    'gast': '0.6.0',\n",
       "    'fsspec': '2024.6.1',\n",
       "    'frozenlist': '1.4.1',\n",
       "    'fqdn': '1.5.1',\n",
       "    'fonttools': '4.53.1',\n",
       "    'flexparser': '0.3.1',\n",
       "    'flexcache': '0.3',\n",
       "    'flatbuffers': '24.3.25',\n",
       "    'filelock': '3.15.4',\n",
       "    'fastjsonschema': '2.20.0',\n",
       "    'executing': '2.0.1',\n",
       "    'exceptiongroup': '1.2.2',\n",
       "    'docutils': '0.20.1',\n",
       "    'docstring-parser': '0.16',\n",
       "    'docker': '7.1.0',\n",
       "    'distributed': '2024.7.1',\n",
       "    'dill': '0.3.8',\n",
       "    'defusedxml': '0.7.1',\n",
       "    'deepspeed': '0.15.2',\n",
       "    'decorator': '5.1.1',\n",
       "    'debugpy': '1.8.2',\n",
       "    'datafed': '3.0.0',\n",
       "    'dask': '2024.7.1',\n",
       "    'cytoolz': '0.12.3',\n",
       "    'cycler': '0.12.1',\n",
       "    'cvxopt': '1.3.2',\n",
       "    'cryptography': '43.0.1',\n",
       "    'contourpy': '1.2.1',\n",
       "    'comm': '0.2.2',\n",
       "    'cloudpickle': '3.0.0',\n",
       "    'click': '8.1.7',\n",
       "    'charset-normalizer': '3.3.2',\n",
       "    'cffi': '1.16.0',\n",
       "    'certifi': '2024.7.4',\n",
       "    'brotli': '1.0.9',\n",
       "    'botorch': '0.12.0',\n",
       "    'bleach': '6.1.0',\n",
       "    'bglib': '0.0.4',\n",
       "    'beautifulsoup4': '4.12.3',\n",
       "    'babel': '2.15.0',\n",
       "    'ax-platform': '0.4.3',\n",
       "    'autophyslearn': '0.2.5',\n",
       "    'autopep8': '2.3.1',\n",
       "    'attrs': '23.2.0',\n",
       "    'async-timeout': '4.0.3',\n",
       "    'async-lru': '2.0.4',\n",
       "    'astunparse': '1.6.3',\n",
       "    'asttokens': '2.4.1',\n",
       "    'asteval': '1.0.1',\n",
       "    'ase': '3.23.0',\n",
       "    'arrow': '1.3.0',\n",
       "    'argon2-cffi': '23.1.0',\n",
       "    'argon2-cffi-bindings': '21.2.0',\n",
       "    'appdirs': '1.4.4',\n",
       "    'anyio': '4.4.0',\n",
       "    'annotated-types': '0.7.0',\n",
       "    'alabaster': '0.7.16',\n",
       "    'aiosignal': '1.3.1',\n",
       "    'aiohttp': '3.10.8',\n",
       "    'aiohappyeyeballs': '2.4.3',\n",
       "    'accessible-pygments': '0.0.5',\n",
       "    'absl-py': '2.1.0'}}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_empty(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clean_empty(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
