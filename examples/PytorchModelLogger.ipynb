{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example with Data Logging in DataFed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from m3util.util.IO import make_folder \n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow/src\"))\n",
    "from datafed_torchflow.pytorch import TorchLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters to Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer for 10 classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "\n",
    "        # Apply fully connected layers with ReLU and final output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations for data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,)\n",
    "        ),  # Normalize with mean and std of MNIST dataset\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model, loss function, and optimizer, and DataFed TorchLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"101024\"\n",
    "notebook_path = (\n",
    "    \"./PytorchModelLogger.ipynb\"\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(SimpleCNN().parameters(), lr=learning_rate)  # Adam optimizer\n",
    "\n",
    "model_dict = {\"model\": SimpleCNN(), \"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to pypi: <Fault -32500: 'RuntimeError: PyPI no longer supports the XMLRPC package_releases method. Use JSON or Simple API instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.'>\n"
     ]
    }
   ],
   "source": [
    "torchlogger = TorchLogger(\n",
    "    model_dict=model_dict,\n",
    "    DataFed_path=f\"2024_test_pytorch/delete_me/{suffix}\",\n",
    "    script_path=notebook_path,\n",
    "    input_data_shape=train_dataset[0][0].shape,\n",
    "    dataset_id_or_path= [file.path for file in os.scandir(\"./data/MNIST/raw\")],\n",
    "    local_model_path=f\"examples/model/{suffix}\",\n",
    "    logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "This function calls TorchLogger.save, which does the following:\n",
    "\n",
    "1. Saves the model checkpoint\n",
    "1. Identifies the approprate metadata for the model (including DataFed provenance dependencies)\n",
    "1. Identifies and navigates to the approprate DataFed project and collection\n",
    "1. Creates a DataFed data record with this metadata\n",
    "1. Saves the model weights file or, gets the local zip file the user specified instead in order to upload multiple files to the same DataFed data record\n",
    "1. Uploads the zip file to the DataFed data record generated in the previous steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epoch,\n",
    "    base_local_file_name,\n",
    "    local_vars,\n",
    "):\n",
    "    make_folder(base_local_file_name)  # ensure the path exists to save the weights\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    file_name = f\"MNSIT_epoch_{epoch}_loss_{loss.item():.4e}\"\n",
    "    local_file_path = f\"{base_local_file_name}/{file_name}.pkl\"\n",
    "\n",
    "    torchlogger.save(\n",
    "        file_name,\n",
    "        epoch=epoch,\n",
    "        #training_loss=loss.item(),\n",
    "        local_file_path=local_file_path,\n",
    "        local_vars=local_vars,\n",
    "        model_hyperparameters={\"learning_rate\": learning_rate},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # Sum up the batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "        f\"({accuracy:.2f}%)\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the DataFed Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test the CNN\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.329736\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.323759\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.267824\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.332277\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.309857\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.312216\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.337153\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.301441\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.289233\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.306317\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 898/10000 (8.98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.312785\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.320899\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.299239\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.303957\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.303349\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.307781\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.305706\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.317954\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.317648\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.330141\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 898/10000 (8.98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.304247\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.303127\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.293526\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.299446\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.311004\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.306405\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.321911\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.324632\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.327349\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.318623\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 898/10000 (8.98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.322479\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.313731\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.314817\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.312050\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.301868\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.319455\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.301650\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.333596\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.298580\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.325087\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 898/10000 (8.98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.330954\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.301117\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.321373\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.294642\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.327962\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.322278\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.312234\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.322147\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.301585\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.319504\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 898/10000 (8.98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    local_vars = locals()\n",
    "\n",
    "    train(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epoch=epoch,\n",
    "        base_local_file_name=\"model/100124/weights\",\n",
    "        local_vars=list(local_vars.items()),\n",
    "    )\n",
    "    test(model=model, device=device, test_loader=test_loader, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
