{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Example with Data Logging in DataFed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "\n",
    "from datafed_torchflow.computer import get_system_info\n",
    "from datafed_torchflow.pytorch import TorchLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters to Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = '/home/jg3837/DataFed_TorchFlow/DataFed_TorchFlow-1/examples/Model_logger.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output layer for 10 classes (digits 0-9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "\n",
    "        # Apply fully connected layers with ReLU and final output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with mean and std of MNIST dataset\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model, loss function, and optimizer, and DataFed TorchLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to pypi: <Fault -32500: 'RuntimeError: PyPI no longer supports the XMLRPC package_releases method. Use JSON or Simple API instead. See https://github.com/pypi/warehouse/issues/16642 and https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.'>\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "torchlogger = TorchLogger(model, \"2024_test_pytorch/delete_me\", optimizer, script_path=notebook_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "            \n",
    "    torchlogger.save(f'MNSIT_epoch_{epoch}_loss_{loss.item():.6f}', epoch=epoch, loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # Sum up the batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({accuracy:.2f}%)\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the DataFed Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.076428\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.001258\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.000468\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.034431\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.000102\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.000859\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.027049\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.013835\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.024749\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.003669\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9921/10000 (99.21%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.009400\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.001928\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.001882\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001530\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.003447\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.001209\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.001096\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.003011\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.000071\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.004439\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9917/10000 (99.17%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.007322\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.000590\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001241\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.081323\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.001110\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.000160\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001000\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.000070\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.000268\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.004218\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9912/10000 (99.12%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.000270\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.000257\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.001081\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.000662\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001007\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000638\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.000013\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.001135\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001162\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.000082\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9887/10000 (98.87%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000065\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.007022\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.000315\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.000398\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.001552\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.023006\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.005044\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.000993\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.002217\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.018248\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9905/10000 (99.05%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test the CNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "n_epochs = 5\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
